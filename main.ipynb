{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "550334bc-2b8d-4dc0-9fb3-ac1089118bba",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Pip Install Libraries"
    }
   },
   "outputs": [],
   "source": [
    "%pip install aiohttp\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf9bbe0b-fe80-4203-80a5-e67afa1b1142",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Workspace Assessment - Main Execution\n",
    "\n",
    "This notebook orchestrates the full workspace assessment process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac51c2c6-b372-495c-b369-164830202e25",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Config"
    }
   },
   "outputs": [],
   "source": [
    "%run \"./config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2d63223-693f-4803-821e-2324c18f32bd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Endpoints"
    }
   },
   "outputs": [],
   "source": [
    "%run \"./endpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5c54b04-b4fa-4c80-8a5e-01d8da91bc1d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Data Processing"
    }
   },
   "outputs": [],
   "source": [
    "%run \"./data_processing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60009ffb-8b32-4bd9-a657-d5e15a29cdf3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Unity Catalog"
    }
   },
   "outputs": [],
   "source": [
    "%run \"./unity_catalog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6064f3b-05e2-4e11-b4da-1615a36671c0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import API Client"
    }
   },
   "outputs": [],
   "source": [
    "%run \"./api_client\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87893386-2cd3-4269-b3ed-20bd8a09786b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Library Imports"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import time\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from datetime import datetime, timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1319c05b-8428-4510-90d6-8bbec77bde05",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Main Execution"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function for the Databricks workspace assessment.\"\"\"\n",
    "    \n",
    "    # Initialize timing and async support\n",
    "    start_time = time.time()\n",
    "    start_ts = datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "    nest_asyncio.apply()\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"[üîê AUTH] Initializing Databricks authentication...\")\n",
    "    \n",
    "    # Get Databricks connection details\n",
    "    # These variables (spark, dbutils) are available in Databricks notebooks\n",
    "    try:\n",
    "        workspace_url = str(spark.conf.get(\"spark.databricks.workspaceUrl\"))\n",
    "        token = str(dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get())\n",
    "        \n",
    "        print(f\"[Init] Connected to workspace: {workspace_url}\")\n",
    "        print(\"=\"*80, \"\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to get Databricks credentials: {e}\")\n",
    "        print(\"Make sure this script is running in a Databricks notebook environment.\")\n",
    "        return\n",
    "    \n",
    "    print(\"[‚ñ∂Ô∏è RUN] Starting full workspace assessment...\")\n",
    "    print(f\"[CONFIG] Streaming writes: {'ENABLED' if ENABLE_STREAMING_WRITES else 'DISABLED'}\")\n",
    "    \n",
    "    # Delete existing data prior to next workspace assessment\n",
    "    if DELETE_EXISTING_DATA: delete_existing_tables(spark, TARGET_CATALOG, TARGET_SCHEMA)\n",
    "    \n",
    "    # Initialize components\n",
    "    data_processor = DataProcessor(spark, workspace_url, start_ts, TARGET_CATALOG, TARGET_SCHEMA)\n",
    "    \n",
    "    # Initialize API client with optional streaming writes support\n",
    "    if ENABLE_STREAMING_WRITES:\n",
    "        # Ensure UC sink exists before starting streaming writes\n",
    "        # data_processor.spark.sql(f\"CREATE CATALOG IF NOT EXISTS `{TARGET_CATALOG}`\")\n",
    "        # data_processor.spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{TARGET_CATALOG}`.`{TARGET_SCHEMA}`\")\n",
    "        \n",
    "        api_client = DatabricksAPIClient(workspace_url, token, data_processor, enable_streaming_writes=True)\n",
    "        print(\"üöÄ Streaming mode: Raw data will be written to UC immediately after each API call\")\n",
    "    else:\n",
    "        api_client = DatabricksAPIClient(workspace_url, token)\n",
    "        print(\"üì¶ Batch mode: Raw data will be collected in memory and written at the end\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Collect REST API data asynchronously\n",
    "        raw_data, rest_counts = asyncio.run(api_client.collect_all_endpoints())\n",
    "        \n",
    "        # Add DBFS mounts (requires dbutils)\n",
    "        mount_data, mount_count = api_client.collect_dbfs_mounts(dbutils)\n",
    "        rest_counts[\"dbfs_mount_points\"] = mount_count\n",
    "        raw_data[\"dbfs_mount_points\"] = mount_data\n",
    "        \n",
    "        # Write DBFS mounts immediately if streaming is enabled\n",
    "        if ENABLE_STREAMING_WRITES and mount_data:\n",
    "            data_processor.write_single_raw_table(\"dbfs_mount_points\", mount_data)\n",
    "        \n",
    "        # Step 2: Unity Catalog enumeration\n",
    "        base_url = f\"https://{workspace_url}\"\n",
    "        headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "        \n",
    "        uc_counts, uc_raw_data = enumerate_uc(\n",
    "            base_url=base_url,\n",
    "            headers=headers,\n",
    "            enable=UC_ENABLE,\n",
    "            allowlist=UC_CATALOG_ALLOWLIST,\n",
    "            catalog_limit=UC_CATALOG_LIMIT,\n",
    "            schema_limit_per_catalog=UC_SCHEMA_LIMIT_PER_CATALOG,\n",
    "            max_workers=UC_MAX_WORKERS,\n",
    "            spark=spark\n",
    "        )\n",
    "        \n",
    "        # Write UC data immediately if streaming is enabled\n",
    "        if uc_raw_data.get(\"schemas\"):\n",
    "            raw_data[\"databricks_schema\"] = uc_raw_data[\"schemas\"]\n",
    "            if ENABLE_STREAMING_WRITES:\n",
    "                data_processor.write_single_raw_table(\"databricks_schema\", uc_raw_data[\"schemas\"])\n",
    "        \n",
    "        if uc_raw_data.get(\"tables\"):\n",
    "            if ENABLE_STREAMING_WRITES:\n",
    "                # Write tables immediately in streaming mode\n",
    "                data_processor.write_single_raw_table(\"databricks_table\", uc_raw_data[\"tables\"])\n",
    "            else:\n",
    "                # Collect for batch mode\n",
    "                raw_data[\"databricks_table\"] = uc_raw_data[\"tables\"]\n",
    "        \n",
    "        # Step 3: Process and write data\n",
    "        if ENABLE_STREAMING_WRITES:\n",
    "            # Raw tables already written during collection, just write summary\n",
    "            print(\"\\n\" + \"=\"*22 + \" 2/4 RAW Data (Already Streamed) \" + \"=\"*22 + \"\\n\")\n",
    "            print(\"[STREAM] Raw tables already written during API collection\")\n",
    "            \n",
    "            # Ensure UC sink exists and write summary\n",
    "            # ensure_uc_sink(TARGET_CATALOG, TARGET_SCHEMA, spark) # Predefine Your Catalog and Schema Before Hand\n",
    "            \n",
    "            # Combine all counts for summary\n",
    "            all_counts = rest_counts.copy()\n",
    "            all_counts.update(uc_counts)\n",
    "            \n",
    "            summary_df = build_and_write_summary(all_counts, TARGET_CATALOG, TARGET_SCHEMA, spark)\n",
    "        else:\n",
    "            # Traditional batch mode - write everything at the end\n",
    "            summary_df = data_processor.process_and_write_all(\n",
    "                raw_data=raw_data,\n",
    "                uc_counts=uc_counts,\n",
    "                catalog=TARGET_CATALOG,\n",
    "                schema=TARGET_SCHEMA\n",
    "            )\n",
    "        \n",
    "        # Step 4: Display results\n",
    "        display(summary_df.orderBy(\"Category\", \"Object\"))\n",
    "        \n",
    "        # Final summary\n",
    "        runtime_min = round((time.time() - start_time) / 60, 2)\n",
    "        total_objects = len(raw_data)\n",
    "        total_records = sum(len(records) for records in raw_data.values())\n",
    "        \n",
    "        print(f\"\\n[‚úÖ DONE] Completed in {runtime_min} min.\")\n",
    "        print(f\"[STATS] Collected {total_objects} object types with {total_records:,} total records\")\n",
    "        print(f\"[STATS] UC: {uc_counts['uc_catalogs']} catalogs, {uc_counts['uc_schemas']} schemas, {uc_counts['uc_tables']} tables\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Assessment failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "# Run the main assessment\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8393953626606947,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "main",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
