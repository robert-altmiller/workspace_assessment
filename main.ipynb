{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "550334bc-2b8d-4dc0-9fb3-ac1089118bba",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Pip Install Libraries"
    }
   },
   "outputs": [],
   "source": [
    "%pip install aiohttp\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf9bbe0b-fe80-4203-80a5-e67afa1b1142",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Workspace Assessment - Main Execution\n",
    "\n",
    "This notebook orchestrates the full workspace assessment process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac51c2c6-b372-495c-b369-164830202e25",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Config"
    }
   },
   "outputs": [],
   "source": [
    "%run \"./config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2d63223-693f-4803-821e-2324c18f32bd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Endpoints"
    }
   },
   "outputs": [],
   "source": [
    "%run \"./endpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5c54b04-b4fa-4c80-8a5e-01d8da91bc1d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Data Processing"
    }
   },
   "outputs": [],
   "source": [
    "%run \"./data_processing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60009ffb-8b32-4bd9-a657-d5e15a29cdf3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Unity Catalog"
    }
   },
   "outputs": [],
   "source": [
    "%run \"./unity_catalog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6064f3b-05e2-4e11-b4da-1615a36671c0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import API Client"
    }
   },
   "outputs": [],
   "source": [
    "%run \"./api_client\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87893386-2cd3-4269-b3ed-20bd8a09786b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Library Imports"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import time\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from datetime import datetime, timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1319c05b-8428-4510-90d6-8bbec77bde05",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Main Execution"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function for the Databricks workspace assessment.\"\"\"\n",
    "    \n",
    "    # Initialize timing and async support\n",
    "    start_time = time.time()\n",
    "    start_ts = datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "    nest_asyncio.apply()\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"[üîê AUTH] Initializing Databricks authentication...\")\n",
    "    \n",
    "    # Get Databricks connection details\n",
    "    # These variables (spark, dbutils) are available in Databricks notebooks\n",
    "    try:\n",
    "        workspace_url = str(spark.conf.get(\"spark.databricks.workspaceUrl\"))\n",
    "        token = str(dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get())\n",
    "        \n",
    "        print(f\"[Init] Connected to workspace: {workspace_url}\")\n",
    "        print(\"=\"*80, \"\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to get Databricks credentials: {e}\")\n",
    "        print(\"Make sure this script is running in a Databricks notebook environment.\")\n",
    "        return\n",
    "    \n",
    "    print(\"[‚ñ∂Ô∏è RUN] Starting full workspace assessment...\")\n",
    "    print(f\"[CONFIG] Streaming writes: {'ENABLED' if ENABLE_STREAMING_WRITES else 'DISABLED'}\")\n",
    "    \n",
    "    # Initialize components\n",
    "    data_processor = DataProcessor(spark, workspace_url, start_ts, TARGET_CATALOG, TARGET_SCHEMA)\n",
    "    \n",
    "    # Initialize API client with optional streaming writes support\n",
    "    if ENABLE_STREAMING_WRITES:\n",
    "        # Ensure UC sink exists before starting streaming writes\n",
    "        # data_processor.spark.sql(f\"CREATE CATALOG IF NOT EXISTS `{TARGET_CATALOG}`\")\n",
    "        # data_processor.spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{TARGET_CATALOG}`.`{TARGET_SCHEMA}`\")\n",
    "        api_client = DatabricksAPIClient(workspace_url, token, data_processor, enable_streaming_writes=True)\n",
    "        print(\"üöÄ Streaming mode: Raw data will be written to UC immediately after each API call\")\n",
    "    else:\n",
    "        api_client = DatabricksAPIClient(workspace_url, token)\n",
    "        print(\"üì¶ Batch mode: Raw data will be collected in memory and written at the end\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Collect REST API data asynchronously\n",
    "        raw_data, rest_counts = asyncio.run(api_client.collect_all_endpoints())\n",
    "        \n",
    "        # Add DBFS mounts (requires dbutils)\n",
    "        mount_data, mount_count = api_client.collect_dbfs_mounts(dbutils)\n",
    "        rest_counts[\"dbfs_mount_points\"] = mount_count\n",
    "        raw_data[\"dbfs_mount_points\"] = mount_data\n",
    "        \n",
    "        # Write DBFS mounts immediately if streaming is enabled\n",
    "        if ENABLE_STREAMING_WRITES and mount_data:\n",
    "            data_processor.write_single_raw_table(\"dbfs_mount_points\", mount_data)\n",
    "        \n",
    "        # Step 2: Unity Catalog enumeration with batch writing\n",
    "        base_url = f\"https://{workspace_url}\"\n",
    "        headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "        \n",
    "        # Define write callback for batch writing UC tables\n",
    "        def write_uc_batch(table_type, records):\n",
    "            \"\"\"Callback to write UC table batches during enumeration.\"\"\"\n",
    "            if ENABLE_STREAMING_WRITES:\n",
    "                data_processor.write_single_raw_table(table_type, records)\n",
    "        \n",
    "        uc_counts, uc_raw_data = enumerate_uc(\n",
    "            base_url=base_url,\n",
    "            headers=headers,\n",
    "            enable=UC_ENABLE,\n",
    "            allowlist=UC_CATALOG_ALLOWLIST,\n",
    "            catalog_limit=UC_CATALOG_LIMIT,\n",
    "            schema_limit_per_catalog=UC_SCHEMA_LIMIT_PER_CATALOG,\n",
    "            max_workers=UC_MAX_WORKERS,\n",
    "            write_callback=write_uc_batch if ENABLE_STREAMING_WRITES else None,\n",
    "            batch_size=UC_TABLE_BATCH_SIZE\n",
    "        )\n",
    "        \n",
    "        # Write UC schema data (tables are already written in batches if streaming)\n",
    "        if uc_raw_data.get(\"schemas\"):\n",
    "            raw_data[\"databricks_schema\"] = uc_raw_data[\"schemas\"]\n",
    "            if ENABLE_STREAMING_WRITES:\n",
    "                data_processor.write_single_raw_table(\"databricks_schema\", uc_raw_data[\"schemas\"])\n",
    "        \n",
    "        # Only write tables in batch mode (streaming mode already wrote them)\n",
    "        if uc_raw_data.get(\"tables\") and not ENABLE_STREAMING_WRITES:\n",
    "            raw_data[\"databricks_table\"] = uc_raw_data[\"tables\"]\n",
    "        \n",
    "        # Step 3: Process and write data\n",
    "        if ENABLE_STREAMING_WRITES:\n",
    "            # Raw tables already written during collection, just write summary\n",
    "            print(\"\\n\" + \"=\"*22 + \" 2/4 RAW Data (Already Streamed) \" + \"=\"*22 + \"\\n\")\n",
    "            print(\"[STREAM] Raw tables already written during API collection\")\n",
    "            \n",
    "            # Ensure UC sink exists and write summary\n",
    "            # ensure_uc_sink(TARGET_CATALOG, TARGET_SCHEMA, spark) # Predefine Your Catalog and Schema Before Hand\n",
    "            \n",
    "            # Combine all counts for summary\n",
    "            all_counts = rest_counts.copy()\n",
    "            all_counts.update(uc_counts)\n",
    "            \n",
    "            summary_df = build_and_write_summary(all_counts, TARGET_CATALOG, TARGET_SCHEMA, spark)\n",
    "        else:\n",
    "            # Traditional batch mode - write everything at the end\n",
    "            summary_df = data_processor.process_and_write_all(\n",
    "                raw_data=raw_data,\n",
    "                uc_counts=uc_counts,\n",
    "                catalog=TARGET_CATALOG,\n",
    "                schema=TARGET_SCHEMA\n",
    "            )\n",
    "        \n",
    "        # Step 4: Display results\n",
    "        display(summary_df.orderBy(\"Category\", \"Object\"))\n",
    "        \n",
    "        # Final summary\n",
    "        runtime_min = round((time.time() - start_time) / 60, 2)\n",
    "        total_objects = len(raw_data)\n",
    "        total_records = sum(len(records) for records in raw_data.values())\n",
    "        \n",
    "        print(f\"\\n[‚úÖ DONE] Completed in {runtime_min} min.\")\n",
    "        print(f\"[STATS] Collected {total_objects} object types with {total_records:,} total records\")\n",
    "        print(f\"[STATS] UC: {uc_counts['uc_catalogs']} catalogs, {uc_counts['uc_schemas']} schemas, {uc_counts['uc_tables']} tables\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Assessment failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "# Run the main assessment\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eed57be-1484-4fdd-b95b-17b1e31731d6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Remove All the Tables"
    }
   },
   "outputs": [],
   "source": [
    "# catalog_name = \"users\"\n",
    "# schema_name = \"robert_altmiller\"\n",
    "\n",
    "# # Get all tables in the catalog and schema\n",
    "# tables_df = spark.sql(f\"SHOW TABLES IN {catalog_name}.{schema_name}\")\n",
    "# tables = [row.tableName for row in tables_df.collect()]\n",
    "\n",
    "# print(f\"Found {len(tables)} tables in {catalog_name}.{schema_name}\")\n",
    "\n",
    "# # Drop all tables\n",
    "# for t in tables:\n",
    "#     fqtn = f\"{catalog_name}.{schema_name}.{t}\"\n",
    "#     print(f\"Dropping {fqtn} ...\")\n",
    "#     spark.sql(f\"DROP TABLE IF EXISTS {fqtn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1425f16f-c88b-4f39-a9ae-5d58c813c7ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # test_jobs_pagination.py\n",
    "# import asyncio\n",
    "# import aiohttp\n",
    "# import os\n",
    "\n",
    "# async def test_jobs_api_real():\n",
    "#     \"\"\"\n",
    "#     Real integration test for Jobs API with pagination.\n",
    "#     Tests actual API calls to verify pagination works correctly.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Get credentials from environment or Databricks context\n",
    "#     workspace_url = os.getenv(\"DATABRICKS_HOST\", \"e2-demo-field-eng.cloud.databricks.com\")\n",
    "#     token = os.getenv(\"DATABRICKS_TOKEN\", str(dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()))\n",
    "    \n",
    "#     if not token:\n",
    "#         # Try to get from dbutils if running in Databricks\n",
    "#         try:\n",
    "#             token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "#         except:\n",
    "#             print(\"‚ùå No token found. Set DATABRICKS_TOKEN environment variable.\")\n",
    "#             return\n",
    "    \n",
    "#     base_url = f\"https://{workspace_url}\"\n",
    "#     headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    \n",
    "#     print(f\"üîó Testing Jobs API: {base_url}/api/2.2/jobs/list\")\n",
    "#     print(\"=\"*60)\n",
    "    \n",
    "#     all_jobs = []\n",
    "#     page_num = 0\n",
    "#     params = {\"limit\": 100}  # Use smaller limit to force pagination\n",
    "    \n",
    "#     async with aiohttp.ClientSession() as session:\n",
    "#         while True:\n",
    "#             page_num += 1\n",
    "#             print(f\"\\nüìÑ Fetching page {page_num}...\")\n",
    "            \n",
    "#             async with session.get(\n",
    "#                 f\"{base_url}/api/2.2/jobs/list\",\n",
    "#                 headers=headers,\n",
    "#                 params=params,\n",
    "#                 timeout=aiohttp.ClientTimeout(total=60)\n",
    "#             ) as resp:\n",
    "#                 if resp.status != 200:\n",
    "#                     print(f\"‚ùå Error: {resp.status} - {await resp.text()}\")\n",
    "#                     break\n",
    "                \n",
    "#                 data = await resp.json()\n",
    "#                 jobs = data.get(\"jobs\", [])\n",
    "#                 next_token = data.get(\"next_page_token\")\n",
    "                \n",
    "#                 print(f\"   ‚úÖ Got {len(jobs)} jobs\")\n",
    "#                 if next_token:\n",
    "#                     print(f\"   üîó Next token: {next_token[:30]}...\")\n",
    "                \n",
    "#                 all_jobs.extend(jobs)\n",
    "                \n",
    "#                 # Continue pagination\n",
    "#                 if next_token:\n",
    "#                     params[\"page_token\"] = next_token\n",
    "#                 else:\n",
    "#                     print(f\"\\n   ‚ÑπÔ∏è  No more pages\")\n",
    "#                     break\n",
    "                \n",
    "#                 # Safety limit\n",
    "#                 # if page_num >= 10:\n",
    "#                 #     print(f\"\\n   ‚ö†Ô∏è  Stopping at page {page_num} (safety limit)\")\n",
    "#                 #     break\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*60)\n",
    "#     print(f\"‚úÖ Test complete!\")\n",
    "#     print(f\"   Total pages fetched: {page_num}\")\n",
    "#     print(f\"   Total jobs collected: {len(all_jobs)}\")\n",
    "    \n",
    "#     if all_jobs:\n",
    "#         print(f\"\\nüìã Sample jobs:\")\n",
    "#         for job in all_jobs[:3]:\n",
    "#             print(f\"   - {job.get('job_id')}: {job.get('settings', {}).get('name', 'N/A')}\")\n",
    "    \n",
    "#     return all_jobs\n",
    "\n",
    "\n",
    "# jobs = asyncio.run(test_jobs_api_real())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8393953626606947,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "main",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
