{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Databricks Workspace Assessment - Main Execution\n",
        "Author: You + GPT-5 Thinking\n",
        "\n",
        "This notebook orchestrates the full workspace assessment process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "# Import our modules\n",
        "from config import (\n",
        "    TARGET_CATALOG, TARGET_SCHEMA, UC_ENABLE, UC_CATALOG_ALLOWLIST,\n",
        "    UC_CATALOG_LIMIT, UC_SCHEMA_LIMIT_PER_CATALOG, UC_MAX_WORKERS,\n",
        "    ENABLE_STREAMING_WRITES\n",
        ")\n",
        "from api_client import DatabricksAPIClient\n",
        "from data_processing import DataProcessor\n",
        "from unity_catalog import enumerate_uc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"\"\"Main execution function for the Databricks workspace assessment.\"\"\"\n",
        "    \n",
        "    # Initialize timing and async support\n",
        "    start_time = time.time()\n",
        "    start_ts = datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
        "    nest_asyncio.apply()\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "    print(\"[üîê AUTH] Initializing Databricks authentication...\")\n",
        "    \n",
        "    # Get Databricks connection details\n",
        "    # These variables (spark, dbutils) are available in Databricks notebooks\n",
        "    try:\n",
        "        workspace_url = str(spark.conf.get(\"spark.databricks.workspaceUrl\"))\n",
        "        token = str(dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get())\n",
        "        \n",
        "        print(f\"[Init] Connected to workspace: {workspace_url}\")\n",
        "        print(\"=\"*80, \"\\n\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Failed to get Databricks credentials: {e}\")\n",
        "        print(\"Make sure this script is running in a Databricks notebook environment.\")\n",
        "        return\n",
        "    \n",
        "    print(\"[‚ñ∂Ô∏è RUN] Starting full workspace assessment...\")\n",
        "    print(f\"[CONFIG] Streaming writes: {'ENABLED' if ENABLE_STREAMING_WRITES else 'DISABLED'}\")\n",
        "    \n",
        "    # Initialize components\n",
        "    data_processor = DataProcessor(spark, workspace_url, start_ts, TARGET_CATALOG, TARGET_SCHEMA)\n",
        "    \n",
        "    # Initialize API client with optional streaming writes support\n",
        "    if ENABLE_STREAMING_WRITES:\n",
        "        # Ensure UC sink exists before starting streaming writes\n",
        "        data_processor.spark.sql(f\"CREATE CATALOG IF NOT EXISTS `{TARGET_CATALOG}`\")\n",
        "        data_processor.spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{TARGET_CATALOG}`.`{TARGET_SCHEMA}`\")\n",
        "        api_client = DatabricksAPIClient(workspace_url, token, data_processor, enable_streaming_writes=True)\n",
        "        print(\"üöÄ Streaming mode: Raw data will be written to UC immediately after each API call\")\n",
        "    else:\n",
        "        api_client = DatabricksAPIClient(workspace_url, token)\n",
        "        print(\"üì¶ Batch mode: Raw data will be collected in memory and written at the end\")\n",
        "    \n",
        "    try:\n",
        "        # Step 1: Collect REST API data asynchronously\n",
        "        raw_data, rest_counts = asyncio.run(api_client.collect_all_endpoints())\n",
        "        \n",
        "        # Add DBFS mounts (requires dbutils)\n",
        "        mount_data, mount_count = api_client.collect_dbfs_mounts(dbutils)\n",
        "        rest_counts[\"dbfs_mount_points\"] = mount_count\n",
        "        raw_data[\"dbfs_mount_points\"] = mount_data\n",
        "        \n",
        "        # Write DBFS mounts immediately if streaming is enabled\n",
        "        if ENABLE_STREAMING_WRITES and mount_data:\n",
        "            data_processor.write_single_raw_table(\"dbfs_mount_points\", mount_data)\n",
        "        \n",
        "        # Step 2: Unity Catalog enumeration\n",
        "        base_url = f\"https://{workspace_url}\"\n",
        "        headers = {\"Authorization\": f\"Bearer {token}\"}\n",
        "        \n",
        "        uc_counts = enumerate_uc(\n",
        "            base_url=base_url,\n",
        "            headers=headers,\n",
        "            enable=UC_ENABLE,\n",
        "            allowlist=UC_CATALOG_ALLOWLIST,\n",
        "            catalog_limit=UC_CATALOG_LIMIT,\n",
        "            schema_limit_per_catalog=UC_SCHEMA_LIMIT_PER_CATALOG,\n",
        "            max_workers=UC_MAX_WORKERS\n",
        "        )\n",
        "        \n",
        "        # Step 3: Process and write data\n",
        "        if ENABLE_STREAMING_WRITES:\n",
        "            # Raw tables already written during collection, just write summary\n",
        "            print(\"\\n\" + \"=\"*22 + \" 2/4 RAW Data (Already Streamed) \" + \"=\"*22 + \"\\n\")\n",
        "            print(\"[STREAM] Raw tables already written during API collection\")\n",
        "            \n",
        "            # Ensure UC sink exists and write summary\n",
        "            from data_processing import ensure_uc_sink, build_and_write_summary\n",
        "            ensure_uc_sink(TARGET_CATALOG, TARGET_SCHEMA, spark)\n",
        "            \n",
        "            # Combine all counts for summary\n",
        "            all_counts = rest_counts.copy()\n",
        "            all_counts.update(uc_counts)\n",
        "            \n",
        "            summary_df = build_and_write_summary(all_counts, TARGET_CATALOG, TARGET_SCHEMA, spark)\n",
        "        else:\n",
        "            # Traditional batch mode - write everything at the end\n",
        "            summary_df = data_processor.process_and_write_all(\n",
        "                raw_data=raw_data,\n",
        "                uc_counts=uc_counts,\n",
        "                catalog=TARGET_CATALOG,\n",
        "                schema=TARGET_SCHEMA\n",
        "            )\n",
        "        \n",
        "        # Step 4: Display results\n",
        "        display(summary_df.orderBy(\"Category\", \"Object\"))\n",
        "        \n",
        "        # Final summary\n",
        "        runtime_min = round((time.time() - start_time) / 60, 2)\n",
        "        total_objects = len(raw_data)\n",
        "        total_records = sum(len(records) for records in raw_data.values())\n",
        "        \n",
        "        print(f\"\\n[‚úÖ DONE] Completed in {runtime_min} min.\")\n",
        "        print(f\"[STATS] Collected {total_objects} object types with {total_records:,} total records\")\n",
        "        print(f\"[STATS] UC: {uc_counts['uc_catalogs']} catalogs, {uc_counts['uc_schemas']} schemas, {uc_counts['uc_tables']} tables\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Assessment failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the main assessment\n",
        "# This cell allows the notebook to be run directly in a Databricks environment\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
