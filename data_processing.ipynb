{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed6b9b09-d428-4289-9f9d-5b1b7a0aaa38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Processing and Unity Catalog Writing Utilities\n",
    "\n",
    "This notebook contains utilities for processing data and writing to Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "272dc2f0-e616-4997-aac6-54f174609d62",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Library Imports"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from typing import Dict, Any, List\n",
    "from datetime import datetime, timezone\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcde4558-eb23-4e08-8223-489b4f7b7d99",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Flatten Objects Function"
    }
   },
   "outputs": [],
   "source": [
    "def banner(txt: str):\n",
    "    print(\"\\n\" + \"=\"*22 + f\" {txt} \" + \"=\"*22 + \"\\n\")\n",
    "\n",
    "def _flatten_obj(x: Any, prefix=\"\") -> Dict[str, Any]:\n",
    "    \"\"\"Flatten nested dict/list into a single-level dict with dotted keys.\"\"\"\n",
    "    out = {}\n",
    "    if isinstance(x, dict):\n",
    "        for k, v in x.items():\n",
    "            kk = f\"{prefix}{k}\" if not prefix else f\"{prefix}.{k}\"\n",
    "            out.update(_flatten_obj(v, kk))\n",
    "    elif isinstance(x, list):\n",
    "        for i, v in enumerate(x):\n",
    "            kk = f\"{prefix}[{i}]\"\n",
    "            out.update(_flatten_obj(v, kk))\n",
    "    else:\n",
    "        out[prefix or \"_value\"] = x\n",
    "    return out\n",
    "\n",
    "def _dedup_columns(cols: List[str]) -> List[str]:\n",
    "    \"\"\"Deduplicate and sanitize column names safely for Spark + Unity Catalog + Delta.\"\"\"\n",
    "    seen = {}\n",
    "    deduped = []\n",
    "    for orig in cols:\n",
    "        # Normalize case\n",
    "        c = orig.lower()\n",
    "        # Replace invalid or risky characters with underscores\n",
    "        c = (c.replace(\".\", \"_\")\n",
    "               .replace(\"[\", \"_\")\n",
    "               .replace(\"]\", \"_\")\n",
    "               .replace(\" \", \"_\")\n",
    "               .replace(\":\", \"_\")\n",
    "               .replace(\"-\", \"_\")\n",
    "               .replace(\"=\", \"_\")\n",
    "               .replace(\"{\", \"_\")\n",
    "               .replace(\"}\", \"_\")\n",
    "               .replace(\"(\", \"_\")\n",
    "               .replace(\")\", \"_\")\n",
    "               .replace(\";\", \"_\")\n",
    "               .replace(\",\", \"_\")\n",
    "               .replace(\"\\\"\", \"_\")\n",
    "               .replace(\"'\", \"_\")\n",
    "               .replace(\"\\n\", \"_\")\n",
    "               .replace(\"\\t\", \"_\"))\n",
    "        # Collapse duplicate underscores and trim\n",
    "        c = re.sub(r\"_+\", \"_\", c).strip(\"_\")\n",
    "        # Deduplicate after normalization\n",
    "        if c in seen:\n",
    "            seen[c] += 1\n",
    "            c = f\"{c}__{seen[c]}\"\n",
    "        else:\n",
    "            seen[c] = 0\n",
    "        deduped.append(c)\n",
    "    return deduped\n",
    "\n",
    "def _convert_complex_to_json(obj: Any, fields_to_preserve: List[str] = None) -> Any:\n",
    "    \"\"\"Convert complex nested fields to JSON strings to avoid excessive flattening.\"\"\"\n",
    "    import json\n",
    "    \n",
    "    if not isinstance(obj, dict):\n",
    "        return obj\n",
    "    \n",
    "    # Default fields that should be kept as JSON strings\n",
    "    if fields_to_preserve is None:\n",
    "        fields_to_preserve = [\n",
    "            'columns', 'properties', 'storage_descriptor', 'partition_keys',\n",
    "            'view_definition', 'table_constraints', 'row_filter', 'column_mask',\n",
    "            'data_source_format', 'table_properties', 'schema_properties',\n",
    "            'catalog_properties', 'storage_location', 'encryption_details',\n",
    "            'delta_runtime_properties_kvpairs', 'effective_predictive_optimization_flag',\n",
    "            'pipelines', 'metastore_id'\n",
    "        ]\n",
    "    \n",
    "    result = {}\n",
    "    for key, value in obj.items():\n",
    "        # Check if this field should be preserved as JSON\n",
    "        if key in fields_to_preserve and isinstance(value, (dict, list)):\n",
    "            # Convert to JSON string\n",
    "            try:\n",
    "                result[f\"{key}_json\"] = json.dumps(value)\n",
    "            except:\n",
    "                result[key] = str(value)\n",
    "        else:\n",
    "            result[key] = value\n",
    "    \n",
    "    return result\n",
    "\n",
    "def _preprocess_uc_records(records: List[Dict[str, Any]], table_type: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Preprocess Unity Catalog records to prevent excessive column explosion.\"\"\"\n",
    "    if table_type == 'databricks_table':\n",
    "        # For UC tables, convert complex nested fields to JSON\n",
    "        return [_convert_complex_to_json(r) for r in records]\n",
    "    elif table_type == 'databricks_schema':\n",
    "        # For UC schemas, keep simple structure (usually already simple)\n",
    "        return [_convert_complex_to_json(r, ['properties', 'schema_properties']) for r in records]\n",
    "    else:\n",
    "        return records\n",
    "\n",
    "def normalize_records(records: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "    \"\"\"Flatten JSON rows to a uniform pandas DataFrame with unique/safe columns.\"\"\"\n",
    "    if not records:\n",
    "        return pd.DataFrame([])\n",
    "    flat_rows = [_flatten_obj(r) for r in records]\n",
    "    df = pd.DataFrame(flat_rows)\n",
    "    df.columns = _dedup_columns(list(df.columns))\n",
    "    return df\n",
    "\n",
    "def to_spark_df(records: List[Dict[str, Any]], spark, table_type: str = None):\n",
    "    \"\"\"Use pandas→spark to avoid sparkContext usage on shared clusters.\"\"\"\n",
    "    if not records:\n",
    "        # Return a more explicit empty DataFrame with proper schema\n",
    "        return spark.createDataFrame([], \"struct<_empty:string>\")\n",
    "    \n",
    "    # Preprocess UC records if table_type is specified\n",
    "    if table_type:\n",
    "        records = _preprocess_uc_records(records, table_type)\n",
    "    \n",
    "    pdf = normalize_records(records)\n",
    "    if pdf.empty:\n",
    "        # If normalization resulted in empty DataFrame, create with basic schema\n",
    "        return spark.createDataFrame([], \"struct<_empty:string>\")\n",
    "    else:\n",
    "        return spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1a33966-2337-478e-8ed5-b4d70fd2ffbb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ensure UC Sink Function"
    }
   },
   "outputs": [],
   "source": [
    "def ensure_uc_sink(catalog: str, schema: str, spark):\n",
    "    \"\"\"Ensure catalog and schema exist in Unity Catalog.\"\"\"\n",
    "    spark.sql(f\"CREATE CATALOG IF NOT EXISTS `{catalog}`\")\n",
    "    spark.sql(f\"CREATE SCHEMA  IF NOT EXISTS `{catalog}`.`{schema}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d4fb42a-191f-4865-8d37-893e0d425fd9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Write Single Raw Table Function"
    }
   },
   "outputs": [],
   "source": [
    "def write_single_raw_table(\n",
    "    key: str,\n",
    "    records: List[Dict[str, Any]],\n",
    "    catalog: str,\n",
    "    schema: str,\n",
    "    workspace_url: str,\n",
    "    start_ts: str,\n",
    "    spark\n",
    "):\n",
    "    \"\"\"Write a single raw data table to Unity Catalog with robust schema handling and batch writing support.\"\"\"\n",
    "    tbl = f\"`{catalog}`.`{schema}`.`raw_{key}`\"\n",
    "    \n",
    "    # Check for empty datasets\n",
    "    if not records:\n",
    "        if SKIP_EMPTY_DATASETS:\n",
    "            print(f\"[SKIP] raw_{key} → {tbl} (empty dataset)\")\n",
    "            return\n",
    "        else:\n",
    "            # Create a minimal empty DataFrame to avoid schema inference errors\n",
    "            empty_df = spark.createDataFrame([], \"struct<_empty:string>\")\n",
    "            empty_df = (empty_df\n",
    "                       .withColumn(\"_collected_at\", F.lit(start_ts))\n",
    "                       .withColumn(\"_workspace\", F.lit(workspace_url)))\n",
    "            try:\n",
    "                empty_df.write.mode(WRITE_RAW_MODE).saveAsTable(tbl)\n",
    "                print(f\"[WRITE] raw_{key} → {tbl} (empty table created)\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                print(f\"[WRITE-FAIL] raw_{key}: Failed to create empty table: {e}\")\n",
    "                return\n",
    "    \n",
    "    # Check if we need batch writing (for very large datasets to avoid protobuf size limits)\n",
    "    total_records = len(records)\n",
    "    use_batch_writing = total_records > WRITE_BATCH_THRESHOLD\n",
    "    \n",
    "    if use_batch_writing:\n",
    "        print(f\"[BATCH-WRITE] raw_{key}: Writing {total_records} rows in batches of {WRITE_BATCH_SIZE}...\")\n",
    "        \n",
    "        # Write in batches\n",
    "        num_batches = (total_records + WRITE_BATCH_SIZE - 1) // WRITE_BATCH_SIZE\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * WRITE_BATCH_SIZE\n",
    "            end_idx = min(start_idx + WRITE_BATCH_SIZE, total_records)\n",
    "            batch_records = records[start_idx:end_idx]\n",
    "            \n",
    "            try:\n",
    "                # Create Spark DataFrame for this batch\n",
    "                sdf = (to_spark_df(batch_records, spark, table_type=key)\n",
    "                       .withColumn(\"_collected_at\", F.lit(start_ts))\n",
    "                       .withColumn(\"_workspace\", F.lit(workspace_url)))\n",
    "                \n",
    "                # First batch: use configured write mode, subsequent batches: append\n",
    "                write_mode = WRITE_RAW_MODE if batch_idx == 0 else \"append\"\n",
    "                writer = sdf.write.mode(write_mode)\n",
    "                \n",
    "                # Add Delta Lake schema evolution options\n",
    "                if ENABLE_MERGE_SCHEMA or batch_idx > 0:  # Always merge schema after first batch\n",
    "                    writer = writer.option(\"mergeSchema\", \"true\")\n",
    "                \n",
    "                if ENABLE_OVERWRITE_SCHEMA and batch_idx == 0:\n",
    "                    writer = writer.option(\"overwriteSchema\", \"true\")\n",
    "                \n",
    "                writer.saveAsTable(tbl)\n",
    "                print(f\"[BATCH {batch_idx+1}/{num_batches}] raw_{key} → {tbl} ({len(batch_records)} rows)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"[WRITE-FAIL] raw_{key} (batch {batch_idx+1}/{num_batches}): {e}\")\n",
    "                if VERBOSE_SCHEMA_ERRORS:\n",
    "                    import traceback\n",
    "                    print(f\"[ERROR-DETAIL] raw_{key} (batch {batch_idx+1}):\")\n",
    "                    print(\"  \" + \"\\n  \".join(traceback.format_exc().split(\"\\n\")[-10:]))\n",
    "                # If first batch fails, stop; otherwise continue with remaining batches\n",
    "                if batch_idx == 0:\n",
    "                    return\n",
    "        \n",
    "        print(f\"[WRITE-COMPLETE] raw_{key} → {tbl} ({total_records} rows total)\")\n",
    "        return\n",
    "    \n",
    "    # Standard single-write path (for smaller datasets)\n",
    "    try:\n",
    "        # Create Spark DataFrame (with preprocessing for UC tables/schemas)\n",
    "        sdf = (to_spark_df(records, spark, table_type=key)\n",
    "               .withColumn(\"_collected_at\", F.lit(start_ts))\n",
    "               .withColumn(\"_workspace\", F.lit(workspace_url)))\n",
    "        \n",
    "        # Configure write options based on schema handling settings\n",
    "        writer = sdf.write.mode(WRITE_RAW_MODE)\n",
    "        \n",
    "        # Add Delta Lake schema evolution options\n",
    "        if ENABLE_MERGE_SCHEMA:\n",
    "            writer = writer.option(\"mergeSchema\", \"true\")\n",
    "        \n",
    "        if ENABLE_OVERWRITE_SCHEMA:\n",
    "            writer = writer.option(\"overwriteSchema\", \"true\")\n",
    "        \n",
    "        # First attempt: try with configured options\n",
    "        try:\n",
    "            writer.saveAsTable(tbl)\n",
    "            print(f\"[WRITE] raw_{key} → {tbl} ({len(records)} rows)\")\n",
    "            return\n",
    "            \n",
    "        except Exception as e1:\n",
    "            error_msg = str(e1).lower()\n",
    "            \n",
    "            # Check if this is a protobuf size error - if so, retry with batch writing\n",
    "            if \"protobuf\" in error_msg or \"negative size\" in error_msg:\n",
    "                print(f\"[PROTOBUF-ERROR] raw_{key}: Message too large, retrying with batch writing...\")\n",
    "                # Recursively call with smaller threshold to force batching\n",
    "                original_threshold = WRITE_BATCH_THRESHOLD\n",
    "                globals()['WRITE_BATCH_THRESHOLD'] = 100  # Force batching\n",
    "                write_single_raw_table(key, records, catalog, schema, workspace_url, start_ts, spark)\n",
    "                globals()['WRITE_BATCH_THRESHOLD'] = original_threshold  # Restore\n",
    "                return\n",
    "            \n",
    "            # Handle specific schema mismatch errors\n",
    "            if any(phrase in error_msg for phrase in [\n",
    "                \"schema mismatch\", \"failed to merge\", \"_legacy_error_temp_delta\",\n",
    "                \"cannot_infer_empty_schema\", \"delta_failed_to_merge_fields\"\n",
    "            ]):\n",
    "                if VERBOSE_SCHEMA_ERRORS:\n",
    "                    print(f\"[SCHEMA-ISSUE] raw_{key}: Schema evolution error detected\")\n",
    "                    print(f\"  Error: {error_msg[:200]}...\")\n",
    "                \n",
    "                # Try fallback strategies\n",
    "                if FALLBACK_TO_OVERWRITE and not ENABLE_OVERWRITE_SCHEMA:\n",
    "                    try:\n",
    "                        print(f\"[RETRY] raw_{key} → Attempting with overwriteSchema=true\")\n",
    "                        fallback_writer = sdf.write.mode(WRITE_RAW_MODE).option(\"overwriteSchema\", \"true\")\n",
    "                        fallback_writer.saveAsTable(tbl)\n",
    "                        print(f\"[WRITE] raw_{key} → {tbl} ({len(records)} rows) [schema overwritten]\")\n",
    "                        return\n",
    "                    except Exception as e2:\n",
    "                        if VERBOSE_SCHEMA_ERRORS:\n",
    "                            print(f\"[SCHEMA-FAIL] raw_{key}: Overwrite schema also failed: {str(e2)[:200]}...\")\n",
    "                \n",
    "                # If schema operations fail and we're in overwrite mode, try dropping the table first\n",
    "                if WRITE_RAW_MODE == \"overwrite\":\n",
    "                    try:\n",
    "                        print(f\"[RETRY] raw_{key} → Attempting to drop and recreate table\")\n",
    "                        spark.sql(f\"DROP TABLE IF EXISTS {tbl}\")\n",
    "                        sdf.write.mode(\"overwrite\").saveAsTable(tbl)\n",
    "                        print(f\"[WRITE] raw_{key} → {tbl} ({len(records)} rows) [table recreated]\")\n",
    "                        return\n",
    "                    except Exception as e3:\n",
    "                        print(f\"[WRITE-FAIL] raw_{key}: All retry attempts failed: {str(e3)[:200]}...\")\n",
    "                        return\n",
    "            \n",
    "            # Re-raise the original exception if it's not a schema issue\n",
    "            raise e1\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"[WRITE-FAIL] raw_{key}: {e}\")\n",
    "        if VERBOSE_SCHEMA_ERRORS:\n",
    "            import traceback\n",
    "            print(f\"[ERROR-DETAIL] raw_{key}:\")\n",
    "            print(\"  \" + \"\\n  \".join(traceback.format_exc().split(\"\\n\")[-10:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69a525e3-1924-47a5-b806-cfbe2059bc87",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Write Raw Tables Function"
    }
   },
   "outputs": [],
   "source": [
    "def write_raw_tables(\n",
    "    raw_dict: Dict[str, List[Dict[str, Any]]],\n",
    "    catalog: str,\n",
    "    schema: str,\n",
    "    workspace_url: str,\n",
    "    start_ts: str,\n",
    "    spark\n",
    "):\n",
    "    \"\"\"Write raw data tables to Unity Catalog.\"\"\"\n",
    "    banner(\"2/4 Write RAW to UC\")\n",
    "    for key, records in raw_dict.items():\n",
    "        write_single_raw_table(key, records, catalog, schema, workspace_url, start_ts, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b62637b0-ad2a-4bcd-bab0-63af118a55ba",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Build and Write Summary Function"
    }
   },
   "outputs": [],
   "source": [
    "def build_and_write_summary(\n",
    "    counts: Dict[str, int],\n",
    "    catalog: str,\n",
    "    schema: str,\n",
    "    spark\n",
    "):\n",
    "    \"\"\"Build and write the summary table with categorized counts.\"\"\"\n",
    "    banner(\"4/4 Write Summary\")\n",
    "    \n",
    "    # UC pieces get human-friendly names\n",
    "    mapping_fixed = {\n",
    "        \"uc_catalogs\": (\"Metastore\", \"UC Catalogs\"),\n",
    "        \"uc_schemas\": (\"Metastore\", \"UC Schemas\"),\n",
    "        \"uc_tables\": (\"Metastore\", \"UC Tables\"),\n",
    "        \"managed_tables\": (\"Metastore\", \"UC Tables (Managed)\"),\n",
    "        \"external_tables\": (\"Metastore\", \"UC Tables (External)\"),\n",
    "        \"dbfs_mount_points\": (\"DBFS/Mounts\", \"Mount Points\"),\n",
    "    }\n",
    "    \n",
    "    # Auto-map for API endpoints\n",
    "    mapping_auto = {}\n",
    "    for k in API_ENDPOINTS.keys():\n",
    "        disp = k.replace(\"databricks_\", \"\").replace(\"_\", \" \").title()\n",
    "        mapping_auto[k] = (\"Auto\", disp)\n",
    "\n",
    "    rows = []\n",
    "    for k, v in counts.items():\n",
    "        cat, obj = mapping_fixed.get(k, mapping_auto.get(k, (\"Other\", k)))\n",
    "        rows.append({\n",
    "            \"Category\": cat,\n",
    "            \"Object\": obj,\n",
    "            \"Count\": int(v),\n",
    "            \"To_be_Migrated\": \"Y\" if cat not in [\"Workspace\", \"MLflow\"] else \"N\"\n",
    "        })\n",
    "\n",
    "    pdf = pd.DataFrame(rows)\n",
    "    sdf = spark.createDataFrame(pdf)\n",
    "    tbl = f\"`{catalog}`.`{schema}`.`workspace_scan_summary`\"\n",
    "    \n",
    "    # Use robust writing for summary table too\n",
    "    try:\n",
    "        writer = sdf.write.mode(WRITE_SUMMARY_MODE)\n",
    "        if ENABLE_MERGE_SCHEMA:\n",
    "            writer = writer.option(\"mergeSchema\", \"true\")\n",
    "        writer.saveAsTable(tbl)\n",
    "        print(f\"[WRITE] summary → {tbl}\")\n",
    "    except Exception as e:\n",
    "        error_msg = str(e).lower()\n",
    "        if \"schema\" in error_msg and FALLBACK_TO_OVERWRITE:\n",
    "            try:\n",
    "                print(f\"[RETRY] summary → Attempting with overwriteSchema=true\")\n",
    "                sdf.write.mode(WRITE_SUMMARY_MODE).option(\"overwriteSchema\", \"true\").saveAsTable(tbl)\n",
    "                print(f\"[WRITE] summary → {tbl} [schema overwritten]\")\n",
    "            except Exception as e2:\n",
    "                print(f\"[WRITE-FAIL] summary: {e2}\")\n",
    "                raise e2\n",
    "        else:\n",
    "            print(f\"[WRITE-FAIL] summary: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f5939c5-9597-4414-b9ec-f27c420f36f6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data Processor Class"
    }
   },
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"Main data processing class for the workspace assessment.\"\"\"\n",
    "    \n",
    "    def __init__(self, spark, workspace_url: str, start_ts: str, catalog: str = None, schema: str = None):\n",
    "        self.spark = spark\n",
    "        self.workspace_url = workspace_url\n",
    "        self.start_ts = start_ts\n",
    "        self.catalog = catalog\n",
    "        self.schema = schema\n",
    "    \n",
    "    def write_single_raw_table(self, key: str, records: List[Dict[str, Any]]):\n",
    "        \"\"\"Write a single raw table immediately.\"\"\"\n",
    "        if not self.catalog or not self.schema:\n",
    "            raise ValueError(\"Catalog and schema must be set for streaming writes\")\n",
    "        \n",
    "        write_single_raw_table(\n",
    "            key, records, self.catalog, self.schema, \n",
    "            self.workspace_url, self.start_ts, self.spark\n",
    "        )\n",
    "    \n",
    "    def process_and_write_all(\n",
    "        self,\n",
    "        raw_data: Dict[str, List[Dict[str, Any]]],\n",
    "        uc_counts: Dict[str, int],\n",
    "        catalog: str,\n",
    "        schema: str\n",
    "    ):\n",
    "        \"\"\"Process and write all data (raw tables + summary).\"\"\"\n",
    "        # Ensure UC destination exists\n",
    "        # ensure_uc_sink(catalog, schema, self.spark) # Predefine Your Catalog and Schema Before Hand\n",
    "        \n",
    "        # Write raw tables\n",
    "        write_raw_tables(raw_data, catalog, schema, self.workspace_url, self.start_ts, self.spark)\n",
    "        \n",
    "        # Combine counts and write summary\n",
    "        all_counts = {key: len(records) for key, records in raw_data.items()}\n",
    "        all_counts.update(uc_counts)\n",
    "        \n",
    "        summary_df = build_and_write_summary(all_counts, catalog, schema, self.spark)\n",
    "        return summary_df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data_processing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
