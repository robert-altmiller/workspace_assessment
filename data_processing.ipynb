{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed6b9b09-d428-4289-9f9d-5b1b7a0aaa38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Processing and Unity Catalog Writing Utilities\n",
    "\n",
    "This notebook contains utilities for processing data and writing to Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "272dc2f0-e616-4997-aac6-54f174609d62",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Library Imports"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from typing import Dict, Any, List\n",
    "from datetime import datetime, timezone\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcde4558-eb23-4e08-8223-489b4f7b7d99",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Flatten Objects Function and Dedup Logic"
    }
   },
   "outputs": [],
   "source": [
    "def banner(txt: str):\n",
    "    print(\"\\n\" + \"=\"*22 + f\" {txt} \" + \"=\"*22 + \"\\n\")\n",
    "\n",
    "def _flatten_obj(x: Any, prefix=\"\") -> Dict[str, Any]:\n",
    "    \"\"\"Flatten nested dict/list into a single-level dict with dotted keys.\"\"\"\n",
    "    out = {}\n",
    "    if isinstance(x, dict):\n",
    "        for k, v in x.items():\n",
    "            kk = f\"{prefix}{k}\" if not prefix else f\"{prefix}.{k}\"\n",
    "            out.update(_flatten_obj(v, kk))\n",
    "    elif isinstance(x, list):\n",
    "        for i, v in enumerate(x):\n",
    "            kk = f\"{prefix}[{i}]\"\n",
    "            out.update(_flatten_obj(v, kk))\n",
    "    else:\n",
    "        out[prefix or \"_value\"] = x\n",
    "    return out\n",
    "\n",
    "def _dedup_columns(cols: List[str]) -> List[str]:\n",
    "    \"\"\"Deduplicate and sanitize column names safely for Spark + Unity Catalog + Delta.\"\"\"\n",
    "    seen = {}\n",
    "    deduped = []\n",
    "    for orig in cols:\n",
    "        # Normalize case\n",
    "        c = orig.lower()\n",
    "        # Replace invalid or risky characters with underscores\n",
    "        c = (c.replace(\".\", \"_\")\n",
    "               .replace(\"[\", \"_\")\n",
    "               .replace(\"]\", \"_\")\n",
    "               .replace(\" \", \"_\")\n",
    "               .replace(\":\", \"_\")\n",
    "               .replace(\"-\", \"_\")\n",
    "               .replace(\"=\", \"_\")\n",
    "               .replace(\"{\", \"_\")\n",
    "               .replace(\"}\", \"_\")\n",
    "               .replace(\"(\", \"_\")\n",
    "               .replace(\")\", \"_\")\n",
    "               .replace(\";\", \"_\")\n",
    "               .replace(\",\", \"_\")\n",
    "               .replace(\"\\\"\", \"_\")\n",
    "               .replace(\"'\", \"_\")\n",
    "               .replace(\"\\n\", \"_\")\n",
    "               .replace(\"\\t\", \"_\"))\n",
    "        # Collapse duplicate underscores and trim\n",
    "        c = re.sub(r\"_+\", \"_\", c).strip(\"_\")\n",
    "        # Deduplicate after normalization\n",
    "        if c in seen:\n",
    "            seen[c] += 1\n",
    "            c = f\"{c}__{seen[c]}\"\n",
    "        else:\n",
    "            seen[c] = 0\n",
    "        deduped.append(c)\n",
    "    return deduped\n",
    "\n",
    "def _convert_all_complex_to_json(obj: Any, keep_scalar_fields: List[str] = None) -> Any:\n",
    "    \"\"\"Convert ALL dict/list fields to JSON strings, keeping only scalar fields.\"\"\"\n",
    "    import json\n",
    "    \n",
    "    if not isinstance(obj, dict):\n",
    "        return obj\n",
    "    \n",
    "    # For UC tables: keep only these simple scalar fields as columns\n",
    "    # Everything else (nested dicts/lists) gets converted to JSON strings\n",
    "    if keep_scalar_fields is None:\n",
    "        keep_scalar_fields = [\n",
    "            # Core identifiers\n",
    "            'name', 'table_name', 'full_name', 'catalog_name', 'schema_name',\n",
    "            'table_id', 'metastore_id',\n",
    "            # Type and format\n",
    "            'table_type', 'data_source_format',\n",
    "            # Ownership and timestamps\n",
    "            'owner', 'created_at', 'created_by', 'updated_at', 'updated_by',\n",
    "            # Status\n",
    "            'comment', 'generation',\n",
    "            # Injected fields\n",
    "            '_catalog', '_schema', '_collected_at', '_workspace'\n",
    "        ]\n",
    "    \n",
    "    result = {}\n",
    "    for key, value in obj.items():\n",
    "        # If value is a dict or list, convert to JSON\n",
    "        if isinstance(value, (dict, list)):\n",
    "            try:\n",
    "                result[f\"{key}_json\"] = json.dumps(value)\n",
    "            except:\n",
    "                result[f\"{key}_json\"] = str(value)\n",
    "        # If value is a simple scalar and key is in whitelist, keep as-is\n",
    "        elif key in keep_scalar_fields or not keep_scalar_fields:\n",
    "            result[key] = value\n",
    "        # Otherwise, convert to string to be safe\n",
    "        else:\n",
    "            result[key] = str(value) if value is not None else None\n",
    "    \n",
    "    return result\n",
    "\n",
    "def _preprocess_uc_records(records: List[Dict[str, Any]], table_type: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Preprocess records to prevent excessive column explosion.\"\"\"\n",
    "    if table_type == 'databricks_table':\n",
    "        # For UC tables, aggressively convert ALL nested structures to JSON\n",
    "        # Only keep simple scalar fields as columns\n",
    "        return [_convert_all_complex_to_json(r) for r in records]\n",
    "    elif table_type == 'databricks_schema':\n",
    "        # For UC schemas, also aggressive conversion\n",
    "        return [_convert_all_complex_to_json(r, keep_scalar_fields=[\n",
    "            'catalog_name', 'schema_name', 'full_name', 'owner', \n",
    "            'created_at', 'updated_at', 'comment', '_catalog', '_schema'\n",
    "        ]) for r in records]\n",
    "    elif table_type == 'databricks_cluster':\n",
    "        # For clusters, keep core scalar fields and convert complex nested fields to JSON\n",
    "        # This prevents column explosion from executors array, spark_conf, tags, etc.\n",
    "        return [_convert_all_complex_to_json(r, keep_scalar_fields=[\n",
    "            # Core identifiers\n",
    "            'cluster_id', 'cluster_name', 'cluster_source',\n",
    "            # State and timing\n",
    "            'state', 'state_message', 'start_time', 'terminated_time', \n",
    "            'last_state_loss_time', 'last_activity_time', 'last_restarted_time',\n",
    "            # Configuration\n",
    "            'spark_version', 'node_type_id', 'driver_node_type_id',\n",
    "            'num_workers', 'autoscale_min_workers', 'autoscale_max_workers',\n",
    "            'autotermination_minutes', 'enable_elastic_disk', 'enable_local_disk_encryption',\n",
    "            # User/ownership\n",
    "            'creator_user_name', 'single_user_name',\n",
    "            # Metadata\n",
    "            'cluster_memory_mb', 'cluster_cores', \n",
    "            'policy_id', 'data_security_mode', 'runtime_engine',\n",
    "            # Injected fields\n",
    "            '_collected_at', '_workspace'\n",
    "        ]) for r in records]\n",
    "    elif table_type == 'databricks_job':\n",
    "        return [_convert_all_complex_to_json(r, keep_scalar_fields=[\n",
    "            'job_id', 'creator_user_name', 'created_time', 'run_as_user_name',\n",
    "            '_collected_at', '_workspace'\n",
    "        ]) for r in records]\n",
    "    elif table_type == 'databricks_pipeline':\n",
    "        return [_convert_all_complex_to_json(r, keep_scalar_fields=[\n",
    "            'pipeline_id', 'name', 'creator_user_name', 'state', 'health',\n",
    "            'latest_updates_state', 'latest_updates_creation_time',\n",
    "            '_collected_at', '_workspace'\n",
    "        ]) for r in records]\n",
    "    elif table_type == 'databricks_model_serving':\n",
    "        return [_convert_all_complex_to_json(r, keep_scalar_fields=[\n",
    "            'id', 'name', 'creator', 'creation_timestamp', 'last_updated_timestamp',\n",
    "            'state_config_served_models', 'state_ready',\n",
    "            '_collected_at', '_workspace'\n",
    "        ]) for r in records]\n",
    "    elif table_type == 'databricks_instance_pool':\n",
    "        return [_convert_all_complex_to_json(r, keep_scalar_fields=[\n",
    "            'instance_pool_id', 'instance_pool_name', 'node_type_id',\n",
    "            'min_idle_instances', 'max_capacity', 'idle_instance_autotermination_minutes',\n",
    "            'state', '_collected_at', '_workspace'\n",
    "        ]) for r in records]\n",
    "    elif table_type == 'databricks_registered_model':\n",
    "        return [_convert_all_complex_to_json(r, keep_scalar_fields=[\n",
    "            'name', 'creation_timestamp', 'last_updated_timestamp', 'user_id',\n",
    "            'id', 'description', '_collected_at', '_workspace'\n",
    "        ]) for r in records]\n",
    "    elif table_type == 'databricks_catalog':\n",
    "        return [_convert_all_complex_to_json(r, keep_scalar_fields=[\n",
    "            'name', 'catalog_type', 'owner', 'comment', 'metastore_id',\n",
    "            'created_at', 'created_by', 'updated_at', 'updated_by',\n",
    "            '_collected_at', '_workspace'\n",
    "        ]) for r in records]\n",
    "    elif table_type == 'databricks_connection':\n",
    "        return [_convert_all_complex_to_json(r, keep_scalar_fields=[\n",
    "            'name', 'connection_type', 'owner', 'comment', 'metastore_id',\n",
    "            'created_at', 'created_by', 'updated_at', 'updated_by',\n",
    "            '_collected_at', '_workspace'\n",
    "        ]) for r in records]\n",
    "    elif table_type == 'databricks_recipient':\n",
    "        return [_convert_all_complex_to_json(r, keep_scalar_fields=[\n",
    "            'name', 'owner', 'comment', 'metastore_id',\n",
    "            'created_at', 'created_by', 'updated_at', 'updated_by',\n",
    "            'authentication_type', 'data_recipient_global_metastore_id',\n",
    "            '_collected_at', '_workspace'\n",
    "        ]) for r in records]\n",
    "    elif table_type == 'databricks_cluster_policy':\n",
    "        return [_convert_all_complex_to_json(r, keep_scalar_fields=[\n",
    "            'policy_id', 'name', 'creator_user_name', 'created_at_timestamp',\n",
    "            'description', 'is_default', 'max_clusters_per_user',\n",
    "            '_collected_at', '_workspace'\n",
    "        ]) for r in records]\n",
    "    elif table_type == 'databricks_experiment':\n",
    "        return [_convert_all_complex_to_json(r, keep_scalar_fields=[\n",
    "            'experiment_id', 'name', 'artifact_location', 'lifecycle_stage',\n",
    "            'creation_time', 'last_update_time',\n",
    "            '_collected_at', '_workspace'\n",
    "        ]) for r in records]\n",
    "    else:\n",
    "        return records\n",
    "\n",
    "def normalize_records(records: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "    \"\"\"Flatten JSON rows to a uniform pandas DataFrame with unique/safe columns.\"\"\"\n",
    "    if not records:\n",
    "        return pd.DataFrame([])\n",
    "    flat_rows = [_flatten_obj(r) for r in records]\n",
    "    df = pd.DataFrame(flat_rows)\n",
    "    df.columns = _dedup_columns(list(df.columns))\n",
    "    return df\n",
    "\n",
    "def to_spark_df(records: List[Dict[str, Any]], spark, table_type: str = None):\n",
    "    \"\"\"Use pandas→spark to avoid sparkContext usage on shared clusters.\"\"\"\n",
    "    if not records:\n",
    "        # Return a more explicit empty DataFrame with proper schema\n",
    "        return spark.createDataFrame([], \"struct<_empty:string>\")\n",
    "    \n",
    "    # Preprocess UC records if table_type is specified\n",
    "    if table_type:\n",
    "        records = _preprocess_uc_records(records, table_type)\n",
    "    \n",
    "    pdf = normalize_records(records)\n",
    "    if pdf.empty:\n",
    "        # If normalization resulted in empty DataFrame, create with basic schema\n",
    "        return spark.createDataFrame([], \"struct<_empty:string>\")\n",
    "    else:\n",
    "        return spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1a33966-2337-478e-8ed5-b4d70fd2ffbb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ensure UC Sink Function"
    }
   },
   "outputs": [],
   "source": [
    "def ensure_uc_sink(catalog: str, schema: str, spark):\n",
    "    \"\"\"Ensure catalog and schema exist in Unity Catalog.\"\"\"\n",
    "    spark.sql(f\"CREATE CATALOG IF NOT EXISTS `{catalog}`\")\n",
    "    spark.sql(f\"CREATE SCHEMA  IF NOT EXISTS `{catalog}`.`{schema}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d4fb42a-191f-4865-8d37-893e0d425fd9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Write Single Raw Table Function"
    }
   },
   "outputs": [],
   "source": [
    "def write_single_raw_table(\n",
    "    key: str,\n",
    "    records: List[Dict[str, Any]],\n",
    "    catalog: str,\n",
    "    schema: str,\n",
    "    workspace_url: str,\n",
    "    start_ts: str,\n",
    "    spark,\n",
    "    mode: str = None\n",
    "):\n",
    "    \"\"\"Write a single raw data table to Unity Catalog with robust schema handling.\n",
    "    \n",
    "    Args:\n",
    "        mode: Write mode ('append' or 'overwrite'). If None, uses WRITE_RAW_MODE from config.\n",
    "    \"\"\"\n",
    "    tbl = f\"`{catalog}`.`{schema}`.`raw_{key}`\"\n",
    "    \n",
    "    # Use explicit mode if provided, otherwise use global config\n",
    "    write_mode = mode if mode is not None else WRITE_RAW_MODE\n",
    "    \n",
    "    # Check for empty datasets\n",
    "    if not records:\n",
    "        if SKIP_EMPTY_DATASETS:\n",
    "            print(f\"[SKIP] raw_{key} → {tbl} (empty dataset)\")\n",
    "            return\n",
    "        else:\n",
    "            # Create a minimal empty DataFrame to avoid schema inference errors\n",
    "            empty_df = spark.createDataFrame([], \"struct<_empty:string>\")\n",
    "            empty_df = (empty_df\n",
    "                       .withColumn(\"_collected_at\", F.lit(start_ts))\n",
    "                       .withColumn(\"_workspace\", F.lit(workspace_url)))\n",
    "            try:\n",
    "                empty_df.write.mode(write_mode).saveAsTable(tbl)\n",
    "                print(f\"[WRITE] raw_{key} → {tbl} (empty table created)\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                print(f\"[WRITE-FAIL] raw_{key}: Failed to create empty table: {e}\")\n",
    "                return\n",
    "    \n",
    "    try:\n",
    "        # Create Spark DataFrame (with preprocessing for UC tables/schemas)\n",
    "        sdf = (to_spark_df(records, spark, table_type=key)\n",
    "               .withColumn(\"_collected_at\", F.lit(start_ts))\n",
    "               .withColumn(\"_workspace\", F.lit(workspace_url)))\n",
    "        \n",
    "        # Configure write options based on schema handling settings\n",
    "        writer = sdf.write.mode(write_mode)\n",
    "        \n",
    "        # Add Delta Lake schema evolution options\n",
    "        if ENABLE_MERGE_SCHEMA:\n",
    "            writer = writer.option(\"mergeSchema\", \"true\")\n",
    "        \n",
    "        if ENABLE_OVERWRITE_SCHEMA:\n",
    "            writer = writer.option(\"overwriteSchema\", \"true\")\n",
    "        \n",
    "        # First attempt: try with configured options\n",
    "        try:\n",
    "            writer.saveAsTable(tbl)\n",
    "            print(f\"[WRITE] raw_{key} → {tbl} ({len(records)} rows)\")\n",
    "            return\n",
    "            \n",
    "        except Exception as e1:\n",
    "            error_msg = str(e1).lower()\n",
    "            \n",
    "            # Handle specific schema mismatch errors\n",
    "            if any(phrase in error_msg for phrase in [\n",
    "                \"schema mismatch\", \"failed to merge\", \"_legacy_error_temp_delta\",\n",
    "                \"cannot_infer_empty_schema\", \"delta_failed_to_merge_fields\"\n",
    "            ]):\n",
    "                if VERBOSE_SCHEMA_ERRORS:\n",
    "                    print(f\"[SCHEMA-ISSUE] raw_{key}: Schema evolution error detected\")\n",
    "                    print(f\"  Error: {error_msg[:200]}...\")\n",
    "                \n",
    "                # Try fallback strategies\n",
    "                if FALLBACK_TO_OVERWRITE and not ENABLE_OVERWRITE_SCHEMA:\n",
    "                    try:\n",
    "                        print(f\"[RETRY] raw_{key} → Attempting with overwriteSchema=true\")\n",
    "                        fallback_writer = sdf.write.mode(write_mode).option(\"overwriteSchema\", \"true\")\n",
    "                        fallback_writer.saveAsTable(tbl)\n",
    "                        print(f\"[WRITE] raw_{key} → {tbl} ({len(records)} rows) [schema overwritten]\")\n",
    "                        return\n",
    "                    except Exception as e2:\n",
    "                        if VERBOSE_SCHEMA_ERRORS:\n",
    "                            print(f\"[SCHEMA-FAIL] raw_{key}: Overwrite schema also failed: {str(e2)[:200]}...\")\n",
    "                \n",
    "                # If schema operations fail and we're in overwrite mode, try dropping the table first\n",
    "                if write_mode == \"overwrite\":\n",
    "                    try:\n",
    "                        print(f\"[RETRY] raw_{key} → Attempting to drop and recreate table\")\n",
    "                        spark.sql(f\"DROP TABLE IF EXISTS {tbl}\")\n",
    "                        sdf.write.mode(\"overwrite\").saveAsTable(tbl)\n",
    "                        print(f\"[WRITE] raw_{key} → {tbl} ({len(records)} rows) [table recreated]\")\n",
    "                        return\n",
    "                    except Exception as e3:\n",
    "                        print(f\"[WRITE-FAIL] raw_{key}: All retry attempts failed: {str(e3)[:200]}...\")\n",
    "                        return\n",
    "            \n",
    "            # Re-raise the original exception if it's not a schema issue\n",
    "            raise e1\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"[WRITE-FAIL] raw_{key}: {e}\")\n",
    "        if VERBOSE_SCHEMA_ERRORS:\n",
    "            import traceback\n",
    "            print(f\"[ERROR-DETAIL] raw_{key}:\")\n",
    "            print(\"  \" + \"\\n  \".join(traceback.format_exc().split(\"\\n\")[-10:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69a525e3-1924-47a5-b806-cfbe2059bc87",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Write Raw Tables Function"
    }
   },
   "outputs": [],
   "source": [
    "def write_raw_tables(\n",
    "    raw_dict: Dict[str, List[Dict[str, Any]]],\n",
    "    catalog: str,\n",
    "    schema: str,\n",
    "    workspace_url: str,\n",
    "    start_ts: str,\n",
    "    spark\n",
    "):\n",
    "    \"\"\"Write raw data tables to Unity Catalog.\"\"\"\n",
    "    banner(\"2/4 Write RAW to UC\")\n",
    "    for key, records in raw_dict.items():\n",
    "        write_single_raw_table(key, records, catalog, schema, workspace_url, start_ts, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b62637b0-ad2a-4bcd-bab0-63af118a55ba",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Build and Write Summary Function"
    }
   },
   "outputs": [],
   "source": [
    "def build_and_write_summary(\n",
    "    counts: Dict[str, int],\n",
    "    catalog: str,\n",
    "    schema: str,\n",
    "    spark\n",
    "):\n",
    "    \"\"\"Build and write the summary table with categorized counts.\"\"\"\n",
    "    banner(\"4/4 Write Summary\")\n",
    "    \n",
    "    # UC pieces get human-friendly names\n",
    "    mapping_fixed = {\n",
    "        \"uc_catalogs\": (\"Metastore\", \"UC Catalogs\"),\n",
    "        \"uc_schemas\": (\"Metastore\", \"UC Schemas\"),\n",
    "        \"uc_tables\": (\"Metastore\", \"UC Tables\"),\n",
    "        \"managed_tables\": (\"Metastore\", \"UC Tables (Managed)\"),\n",
    "        \"external_tables\": (\"Metastore\", \"UC Tables (External)\"),\n",
    "        \"dbfs_mount_points\": (\"DBFS/Mounts\", \"Mount Points\"),\n",
    "    }\n",
    "    \n",
    "    # Auto-map for API endpoints\n",
    "    mapping_auto = {}\n",
    "    for k in API_ENDPOINTS.keys():\n",
    "        disp = k.replace(\"databricks_\", \"\").replace(\"_\", \" \").title()\n",
    "        mapping_auto[k] = (\"Auto\", disp)\n",
    "\n",
    "    rows = []\n",
    "    for k, v in counts.items():\n",
    "        cat, obj = mapping_fixed.get(k, mapping_auto.get(k, (\"Other\", k)))\n",
    "        rows.append({\n",
    "            \"Category\": cat,\n",
    "            \"Object\": obj,\n",
    "            \"Count\": int(v),\n",
    "            \"To_be_Migrated\": \"Y\" if cat not in [\"Workspace\", \"MLflow\"] else \"N\"\n",
    "        })\n",
    "\n",
    "    pdf = pd.DataFrame(rows)\n",
    "    sdf = spark.createDataFrame(pdf)\n",
    "    tbl = f\"`{catalog}`.`{schema}`.`workspace_scan_summary`\"\n",
    "    \n",
    "    # Use robust writing for summary table too\n",
    "    try:\n",
    "        writer = sdf.write.mode(WRITE_SUMMARY_MODE)\n",
    "        if ENABLE_MERGE_SCHEMA:\n",
    "            writer = writer.option(\"mergeSchema\", \"true\")\n",
    "        writer.saveAsTable(tbl)\n",
    "        print(f\"[WRITE] summary → {tbl}\")\n",
    "    except Exception as e:\n",
    "        error_msg = str(e).lower()\n",
    "        if \"schema\" in error_msg and FALLBACK_TO_OVERWRITE:\n",
    "            try:\n",
    "                print(f\"[RETRY] summary → Attempting with overwriteSchema=true\")\n",
    "                sdf.write.mode(WRITE_SUMMARY_MODE).option(\"overwriteSchema\", \"true\").saveAsTable(tbl)\n",
    "                print(f\"[WRITE] summary → {tbl} [schema overwritten]\")\n",
    "            except Exception as e2:\n",
    "                print(f\"[WRITE-FAIL] summary: {e2}\")\n",
    "                raise e2\n",
    "        else:\n",
    "            print(f\"[WRITE-FAIL] summary: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_existing_tables(\n",
    "    spark, \n",
    "    catalog_name: str, \n",
    "    schema_name: str,\n",
    "    table_patterns: List[str] = None,\n",
    "    exclude_patterns: List[str] = None\n",
    "):\n",
    "    \"\"\"Delete assessment tables from catalog/schema with flexible pattern matching.\"\"\"\n",
    "    \n",
    "    # Default patterns if none provided\n",
    "    if table_patterns is None:\n",
    "        table_patterns = [\"raw_databricks_\", \"workspace_scan_summary\"]\n",
    "    \n",
    "    if exclude_patterns is None:\n",
    "        exclude_patterns = []\n",
    "    \n",
    "    # Get all tables in the catalog and schema\n",
    "    try:\n",
    "        tables_df = spark.sql(f\"SHOW TABLES IN `{catalog_name}`.`{schema_name}`\")\n",
    "        tables = [row.tableName for row in tables_df.collect()]\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to list tables in {catalog_name}.{schema_name}: {e}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"[DELETE] Found {len(tables)} tables in {catalog_name}.{schema_name}\")\n",
    "    \n",
    "    # Filter tables based on patterns\n",
    "    tables_to_delete = []\n",
    "    for t in tables:\n",
    "        matches_include = any(pattern in t for pattern in table_patterns)\n",
    "        matches_exclude = any(pattern in t for pattern in exclude_patterns)\n",
    "        \n",
    "        if matches_include and not matches_exclude:\n",
    "            tables_to_delete.append(t)\n",
    "    \n",
    "    if not tables_to_delete:\n",
    "        print(f\"[DELETE] No tables matched the specified patterns\")\n",
    "        return\n",
    "    \n",
    "    # Delete tables\n",
    "    print(f\"[DELETE] Deleting {len(tables_to_delete)} tables...\")\n",
    "    deleted_count = 0\n",
    "    failed_count = 0\n",
    "    \n",
    "    for t in tables_to_delete:\n",
    "        fqtn = f\"`{catalog_name}`.`{schema_name}`.`{t}`\"\n",
    "        try:\n",
    "            spark.sql(f\"DROP TABLE IF EXISTS {fqtn}\")\n",
    "            print(f\"  ✓ Dropped {fqtn}\")\n",
    "            deleted_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Failed to drop {fqtn}: {e}\")\n",
    "            failed_count += 1\n",
    "    \n",
    "    print(f\"[DELETE] ✅ Deleted {deleted_count} tables\")\n",
    "    if failed_count > 0:\n",
    "        print(f\"[DELETE] ❌ Failed to delete {failed_count} tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f5939c5-9597-4414-b9ec-f27c420f36f6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data Processor Class"
    }
   },
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"Main data processing class for the workspace assessment.\"\"\"\n",
    "    \n",
    "    def __init__(self, spark, workspace_url: str, start_ts: str, catalog: str = None, schema: str = None):\n",
    "        self.spark = spark\n",
    "        self.workspace_url = workspace_url\n",
    "        self.start_ts = start_ts\n",
    "        self.catalog = catalog\n",
    "        self.schema = schema\n",
    "    \n",
    "    def write_single_raw_table(self, key: str, records: List[Dict[str, Any]], mode: str = None):\n",
    "        \"\"\"Write a single raw table immediately.\n",
    "        \n",
    "        Args:\n",
    "            mode: Write mode ('append' or 'overwrite'). If None, uses WRITE_RAW_MODE from config.\n",
    "        \"\"\"\n",
    "        if not self.catalog or not self.schema:\n",
    "            raise ValueError(\"Catalog and schema must be set for streaming writes\")\n",
    "        \n",
    "        write_single_raw_table(\n",
    "            key, records, self.catalog, self.schema, \n",
    "            self.workspace_url, self.start_ts, self.spark, mode\n",
    "        )\n",
    "    \n",
    "    def process_and_write_all(\n",
    "        self,\n",
    "        raw_data: Dict[str, List[Dict[str, Any]]],\n",
    "        uc_counts: Dict[str, int],\n",
    "        catalog: str,\n",
    "        schema: str\n",
    "    ):\n",
    "        \"\"\"Process and write all data (raw tables + summary).\"\"\"\n",
    "        # Ensure UC destination exists\n",
    "        # ensure_uc_sink(catalog, schema, self.spark) # Predefine Your Catalog and Schema Before Hand\n",
    "        \n",
    "        # Write raw tables\n",
    "        write_raw_tables(raw_data, catalog, schema, self.workspace_url, self.start_ts, self.spark)\n",
    "        \n",
    "        # Combine counts and write summary\n",
    "        all_counts = {key: len(records) for key, records in raw_data.items()}\n",
    "        all_counts.update(uc_counts)\n",
    "        \n",
    "        summary_df = build_and_write_summary(all_counts, catalog, schema, self.spark)\n",
    "        return summary_df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data_processing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
