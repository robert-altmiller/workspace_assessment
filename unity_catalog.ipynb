{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58c9ed36-a3a6-46b7-8bea-e4626061aeac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Unity Catalog Enumeration Logic\n",
    "\n",
    "This notebook provides utilities for enumerating Unity Catalog objects (catalogs, schemas, tables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "026c25fe-4cb6-4cb5-b213-1e80949d8878",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Library Imports"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import requests\n",
    "import threading\n",
    "from typing import Dict, List, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Catalog processing configuration\n",
    "CATALOG_BATCH_SIZE = 2           # Process n > 0 catalogs in parallel (API calls) - DON'T MODIFY\n",
    "CATALOG_WRITE_BATCH = 3          # Accumulate 100 catalogs before writing (Delta writes) - DON'T MODIFY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83a2dc85-74ac-47a2-96f0-b9c1055abea2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Verbose Logging Functions"
    }
   },
   "outputs": [],
   "source": [
    "def log(msg: str):\n",
    "    if VERBOSE_LOG:\n",
    "        print(msg)\n",
    "\n",
    "def banner(txt: str):\n",
    "    print(\"\\n\" + \"=\"*22 + f\" {txt} \" + \"=\"*22 + \"\\n\")\n",
    "\n",
    "def keep_spark_alive(spark, stop_event, interval=30):\n",
    "    \"\"\"\n",
    "    Keep Spark session alive by running simple queries periodically.\n",
    "    Run this in a background thread during long operations to prevent session timeout.\n",
    "    \n",
    "    Args:\n",
    "        spark: SparkSession instance\n",
    "        stop_event: threading.Event to signal when to stop\n",
    "        interval: Seconds between keep-alive pings (default: 30)\n",
    "    \"\"\"\n",
    "    ping_count = 0\n",
    "    while not stop_event.is_set():\n",
    "        try:\n",
    "            # Simple operation to keep session alive\n",
    "            df = spark.sql(\"SELECT 1\")\n",
    "            df_count = df.count()\n",
    "            ping_count += 1\n",
    "            print(f\"[KEEP-ALIVE] ✓ Ping #{ping_count} successful (interval: {interval}s)\")\n",
    "            time.sleep(interval)\n",
    "        except Exception as e:\n",
    "            if VERBOSE_LOG:\n",
    "                print(f\"[KEEP-ALIVE] ✗ Warning: {e}\")\n",
    "            time.sleep(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2472967e-9773-4b45-89a5-4b07aaf17d1f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Unity Catalog Enumerator Class"
    }
   },
   "outputs": [],
   "source": [
    "class UnityCatalogEnumerator:\n",
    "    \"\"\"Unity Catalog enumeration with threading and limits.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, headers: Dict[str, str]):\n",
    "        self.base_url = base_url\n",
    "        self.headers = headers\n",
    "    \n",
    "    def list_tables_for_schema(self, catalog: str, schema: str) -> Tuple[int, int, int]:\n",
    "        \"\"\"Return (tables, managed, external) for catalog.schema\"\"\"\n",
    "        try:\n",
    "            r = requests.get(\n",
    "                f\"{self.base_url}/api/2.1/unity-catalog/tables\",\n",
    "                headers=self.headers,\n",
    "                params={\"catalog_name\": catalog, \"schema_name\": schema},\n",
    "                timeout=HTTP_TIMEOUT_SEC\n",
    "            )\n",
    "            if r.status_code == 200:\n",
    "                tables = r.json().get(\"tables\", [])\n",
    "                managed = sum(1 for t in tables if t.get(\"table_type\") == \"MANAGED\")\n",
    "                external = sum(1 for t in tables if t.get(\"table_type\") == \"EXTERNAL\")\n",
    "                return len(tables), managed, external\n",
    "        except Exception as e:\n",
    "            log(f\"[UC-ERROR] {catalog}.{schema}: {e}\")\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    def list_tables_for_schema_with_data(self, catalog: str, schema: str) -> List[Dict[str, any]]:\n",
    "        \"\"\"Return list of table metadata for catalog.schema with pagination support\"\"\"\n",
    "        try:\n",
    "            all_tables = []\n",
    "            next_page_token = None\n",
    "            page_count = 0\n",
    "            \n",
    "            while True:\n",
    "                params = {\"catalog_name\": catalog, \"schema_name\": schema, \"max_results\": 500}\n",
    "                if next_page_token:\n",
    "                    params[\"page_token\"] = next_page_token\n",
    "                    \n",
    "                r = requests.get(\n",
    "                    f\"{self.base_url}/api/2.1/unity-catalog/tables\",\n",
    "                    headers=self.headers,\n",
    "                    params=params,\n",
    "                    timeout=HTTP_TIMEOUT_SEC\n",
    "                )\n",
    "                \n",
    "                if r.status_code == 200:\n",
    "                    data = r.json()\n",
    "                    tables = data.get(\"tables\", [])\n",
    "                    page_count += 1\n",
    "                    \n",
    "                    # Add catalog and schema context to each table\n",
    "                    for tbl in tables:\n",
    "                        tbl[\"_catalog\"] = catalog\n",
    "                        tbl[\"_schema\"] = schema\n",
    "                    \n",
    "                    all_tables.extend(tables)\n",
    "                    \n",
    "                    # Check for next page\n",
    "                    next_page_token = data.get(\"next_page_token\")\n",
    "                    if not next_page_token:\n",
    "                        break  # No more pages\n",
    "                    \n",
    "                    log(f\"[UC-PAGINATION] {catalog}.{schema}: page {page_count} fetched {len(tables)} tables, continuing...\")\n",
    "                else:\n",
    "                    log(f\"[UC-SKIP] {catalog}.{schema}: HTTP {r.status_code}\")\n",
    "                    break\n",
    "                    \n",
    "            if page_count > 1:\n",
    "                log(f\"[UC-PAGINATION] {catalog}.{schema}: completed {page_count} pages, total {len(all_tables)} tables\")\n",
    "            return all_tables\n",
    "        except Exception as e:\n",
    "            log(f\"[UC-ERROR] {catalog}.{schema}: {e}\")\n",
    "        return []\n",
    "    \n",
    "    def get_catalogs(self, allowlist: List[str] = None, catalog_limit: int = 0) -> List[str]:\n",
    "        \"\"\"Get list of catalogs to scan.\"\"\"\n",
    "        try:\n",
    "            rr = requests.get(f\"{self.base_url}/api/2.1/unity-catalog/catalogs\", \n",
    "                            headers=self.headers, timeout=HTTP_TIMEOUT_SEC)\n",
    "            rr.raise_for_status()\n",
    "            catalogs = [c[\"name\"] for c in rr.json().get(\"catalogs\", [])]\n",
    "            \n",
    "            if allowlist:\n",
    "                # Special case: if hive_metastore is in allowlist, always include it\n",
    "                # (it's not returned by catalogs API but exists in legacy metastore)\n",
    "                if \"hive_metastore\" in allowlist and \"hive_metastore\" not in catalogs:\n",
    "                    catalogs.append(\"hive_metastore\")\n",
    "                    log(\"[UC] Added legacy hive_metastore to catalog list\")\n",
    "                \n",
    "                catset = set(catalogs)\n",
    "                catalogs = [c for c in allowlist if c in catset]\n",
    "            \n",
    "            catalogs = sorted(catalogs)\n",
    "            \n",
    "            if catalog_limit and catalog_limit > 0:\n",
    "                catalogs = catalogs[:catalog_limit]\n",
    "                \n",
    "            return catalogs\n",
    "        except Exception as e:\n",
    "            log(f\"[UC-ERROR] Failed to get catalogs: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_schemas_for_catalog(self, catalog: str, schema_limit: int = 0) -> List[str]:\n",
    "        \"\"\"Get list of schemas for a catalog with pagination support.\"\"\"\n",
    "        try:\n",
    "            all_schemas = []\n",
    "            next_page_token = None\n",
    "            page_count = 0\n",
    "            \n",
    "            while True:\n",
    "                params = {\"catalog_name\": catalog, \"max_results\": 500}\n",
    "                if next_page_token:\n",
    "                    params[\"page_token\"] = next_page_token\n",
    "                    \n",
    "                rs = requests.get(\n",
    "                    f\"{self.base_url}/api/2.1/unity-catalog/schemas\",\n",
    "                    headers=self.headers,\n",
    "                    params=params,\n",
    "                    timeout=HTTP_TIMEOUT_SEC\n",
    "                )\n",
    "                \n",
    "                if rs.status_code != 200:\n",
    "                    log(f\"[UC] Skip catalog {catalog} (status={rs.status_code})\")\n",
    "                    return []\n",
    "                \n",
    "                data = rs.json()\n",
    "                schemas = [s[\"name\"] for s in data.get(\"schemas\", [])]\n",
    "                page_count += 1\n",
    "                all_schemas.extend(schemas)\n",
    "                \n",
    "                # Check for next page\n",
    "                next_page_token = data.get(\"next_page_token\")\n",
    "                if not next_page_token:\n",
    "                    break  # No more pages\n",
    "                \n",
    "                log(f\"[UC-PAGINATION] {catalog}: page {page_count} fetched {len(schemas)} schemas, continuing...\")\n",
    "            \n",
    "            if page_count > 1:\n",
    "                log(f\"[UC-PAGINATION] {catalog}: completed {page_count} pages, total {len(all_schemas)} schemas\")\n",
    "            \n",
    "            all_schemas = sorted(all_schemas)\n",
    "            \n",
    "            if schema_limit and schema_limit > 0:\n",
    "                all_schemas = all_schemas[:schema_limit]\n",
    "                \n",
    "            return all_schemas\n",
    "        except Exception as e:\n",
    "            log(f\"[UC-ERROR] Failed to get schemas for {catalog}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def enumerate_unity_catalog(\n",
    "        self,\n",
    "        enable: bool = True,\n",
    "        allowlist: List[str] = None,\n",
    "        catalog_limit: int = 0,\n",
    "        schema_limit_per_catalog: int = 0,\n",
    "        max_workers: int = 20,\n",
    "        spark = None,\n",
    "        write_callback = None\n",
    "    ) -> Tuple[Dict[str, int], Dict[str, List[Dict[str, any]]]]:\n",
    "        \"\"\"\n",
    "        Enumerate Unity Catalog with configurable limits and threading.\n",
    "        \n",
    "        Args:\n",
    "            write_callback: Optional function(table_type, records) to write data per catalog\n",
    "        \n",
    "        Returns tuple of (counts, raw_data):\n",
    "        - counts: dict with uc_catalogs, uc_schemas, uc_tables, managed_tables, external_tables\n",
    "        - raw_data: dict with \"schemas\" and \"tables\" lists containing full metadata\n",
    "        \"\"\"\n",
    "        banner(\"3/4 Unity Catalog Enumeration\")\n",
    "        out = {\n",
    "            \"uc_catalogs\": 0,\n",
    "            \"uc_schemas\": 0, \n",
    "            \"uc_tables\": 0,\n",
    "            \"managed_tables\": 0,\n",
    "            \"external_tables\": 0\n",
    "        }\n",
    "        \n",
    "        # Collect actual data\n",
    "        raw_data = {\n",
    "            \"schemas\": [],\n",
    "            \"tables\": []\n",
    "        }\n",
    "        \n",
    "        if not enable:\n",
    "            print(\"[UC] Skipped (UC_ENABLE=False).\")\n",
    "            return out, raw_data\n",
    "\n",
    "        # Note: Spark keep-alive is now managed globally in main.ipynb\n",
    "        \n",
    "        try:\n",
    "            allowlist = allowlist or []\n",
    "            \n",
    "            # Get catalogs to scan\n",
    "            catalogs = self.get_catalogs(allowlist, catalog_limit)\n",
    "            if not catalogs:\n",
    "                print(\"[UC] No catalogs found or accessible.\")\n",
    "                return out, raw_data\n",
    "                \n",
    "            out[\"uc_catalogs\"] = len(catalogs)\n",
    "\n",
    "            print(f\"[UC] Scope: allowlist={allowlist or 'ALL'}, cat_limit={catalog_limit or 'ALL'}, schema_limit={schema_limit_per_catalog or 'ALL'}\")\n",
    "            print(f\"[UC] Catalogs to scan: {len(catalogs)} → {catalogs[:5]}{' …' if len(catalogs)>5 else ''}\")\n",
    "\n",
    "            total_schemas = total_tables = managed = external = 0\n",
    "            accumulated_tables = []  # Accumulate tables across multiple catalogs\n",
    "            catalogs_since_last_write = 0\n",
    "\n",
    "            with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "                # Process catalogs in BATCHES for speed + memory balance\n",
    "                for batch_start in range(0, len(catalogs), CATALOG_BATCH_SIZE):\n",
    "                    batch_end = min(batch_start + CATALOG_BATCH_SIZE, len(catalogs))\n",
    "                    catalog_batch = catalogs[batch_start:batch_end]\n",
    "                    batch_num = (batch_start // CATALOG_BATCH_SIZE) + 1\n",
    "                    total_batches = (len(catalogs) + CATALOG_BATCH_SIZE - 1) // CATALOG_BATCH_SIZE\n",
    "                    \n",
    "                    print(f\"\\n[UC] Processing API batch {batch_num}/{total_batches}: catalogs {batch_start+1}-{batch_end}\")\n",
    "                    \n",
    "                    # Track all futures for this batch\n",
    "                    batch_futures = {}  # {future: catalog_name}\n",
    "                    batch_tables_by_catalog = {}  # {catalog_name: [tables]}\n",
    "                    \n",
    "                    # Queue ALL schemas/tables for ALL catalogs in this batch\n",
    "                    for cat in catalog_batch:\n",
    "                        schemas = self.get_schemas_for_catalog(cat, schema_limit_per_catalog)\n",
    "                        total_schemas += len(schemas)\n",
    "                        print(f\"[UC]   {cat}: {len(schemas)} schemas\")\n",
    "                        \n",
    "                        batch_tables_by_catalog[cat] = []\n",
    "                        \n",
    "                        for s in schemas:\n",
    "                            raw_data[\"schemas\"].append({\n",
    "                                \"catalog_name\": cat,\n",
    "                                \"schema_name\": s,\n",
    "                                \"full_name\": f\"{cat}.{s}\"\n",
    "                            })\n",
    "                            future = ex.submit(self.list_tables_for_schema_with_data, cat, s)\n",
    "                            batch_futures[future] = cat\n",
    "                            time.sleep(0.001)  # Reduced delay since we're batching\n",
    "                    \n",
    "                    # Process results for entire batch\n",
    "                    for future in as_completed(batch_futures):\n",
    "                        cat = batch_futures[future]\n",
    "                        table_data = future.result()\n",
    "                        \n",
    "                        if table_data:\n",
    "                            batch_tables_by_catalog[cat].extend(table_data)\n",
    "                            \n",
    "                            # Count stats\n",
    "                            for tbl in table_data:\n",
    "                                total_tables += 1\n",
    "                                if tbl.get(\"table_type\") == \"MANAGED\":\n",
    "                                    managed += 1\n",
    "                                elif tbl.get(\"table_type\") == \"EXTERNAL\":\n",
    "                                    external += 1\n",
    "                    \n",
    "                    # Accumulate tables from this batch\n",
    "                    for cat in catalog_batch:\n",
    "                        catalog_tables = batch_tables_by_catalog[cat]\n",
    "                        if catalog_tables:\n",
    "                            if write_callback:\n",
    "                                # Add to accumulator instead of writing immediately\n",
    "                                accumulated_tables.extend(catalog_tables)\n",
    "                                catalogs_since_last_write += 1\n",
    "                            else:\n",
    "                                # Batch mode - collect all\n",
    "                                raw_data[\"tables\"].extend(catalog_tables)\n",
    "                        \n",
    "                        del catalog_tables\n",
    "                    \n",
    "                    del batch_tables_by_catalog\n",
    "                    del batch_futures\n",
    "                    \n",
    "                    # Write every CATALOG_WRITE_BATCH catalogs (or at the end)\n",
    "                    if write_callback and accumulated_tables:\n",
    "                        should_write = (catalogs_since_last_write >= CATALOG_WRITE_BATCH) or (batch_end == len(catalogs))\n",
    "                        \n",
    "                        if should_write:\n",
    "                            print(f\"[UC] Writing {len(accumulated_tables)} tables from {catalogs_since_last_write} catalogs\")\n",
    "                            write_callback(\"databricks_table\", accumulated_tables)\n",
    "                            accumulated_tables.clear()\n",
    "                            catalogs_since_last_write = 0\n",
    "                            gc.collect()  # Garbage collect after write\n",
    "                    \n",
    "                    # Progress update\n",
    "                    if batch_num % 5 == 0 or batch_end == len(catalogs):\n",
    "                        print(f\"[UC] Progress: {batch_end}/{len(catalogs)} catalogs processed, {total_tables} total tables\")\n",
    "\n",
    "            out[\"uc_schemas\"] = total_schemas\n",
    "            out[\"uc_tables\"] = total_tables\n",
    "            out[\"managed_tables\"] = managed\n",
    "            out[\"external_tables\"] = external\n",
    "\n",
    "            # Build scope description\n",
    "            scope = []\n",
    "            if allowlist: \n",
    "                scope.append(f\"allow={allowlist}\")\n",
    "            if catalog_limit: \n",
    "                scope.append(f\"cat_limit={catalog_limit}\")\n",
    "            if schema_limit_per_catalog: \n",
    "                scope.append(f\"schema_limit={schema_limit_per_catalog}\")\n",
    "            desc = \", \".join(scope) if scope else \"full-scan\"\n",
    "            \n",
    "            print(f\"[UC] Done ({desc}) → {out['uc_catalogs']} catalogs, {out['uc_schemas']} schemas, {out['uc_tables']} tables\")\n",
    "            print(f\"[UC] Collected {len(raw_data['schemas'])} schema records, {len(raw_data['tables'])} table records\")\n",
    "            return out, raw_data\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"[UC-ERROR] Enumeration failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return out, raw_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1ac6018-416a-48ce-a48d-5aa47cc878e5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Enumerate UC Function"
    }
   },
   "outputs": [],
   "source": [
    "# Convenience function for backward compatibility\n",
    "def enumerate_uc(\n",
    "    base_url: str,\n",
    "    headers: Dict[str, str],\n",
    "    enable: bool = UC_ENABLE,\n",
    "    allowlist: List[str] = None,\n",
    "    catalog_limit: int = UC_CATALOG_LIMIT,\n",
    "    schema_limit_per_catalog: int = UC_SCHEMA_LIMIT_PER_CATALOG,\n",
    "    max_workers: int = UC_MAX_WORKERS,\n",
    "    spark = None,\n",
    "    write_callback = None\n",
    ") -> Tuple[Dict[str, int], Dict[str, List[Dict[str, any]]]]:\n",
    "    \"\"\"\n",
    "    Wrapper for Unity Catalog enumeration.\n",
    "    \n",
    "    Args:\n",
    "        spark: SparkSession to keep alive during long enumeration\n",
    "        write_callback: Optional function(table_type, records) to write data per catalog\n",
    "    \n",
    "    Returns tuple of (counts, raw_data):\n",
    "    - counts: dict with catalog/schema/table counts\n",
    "    - raw_data: dict with \"schemas\" and \"tables\" lists containing full metadata\n",
    "    \"\"\"\n",
    "    enumerator = UnityCatalogEnumerator(base_url, headers)\n",
    "    return enumerator.enumerate_unity_catalog(\n",
    "        enable=enable,\n",
    "        allowlist=allowlist or UC_CATALOG_ALLOWLIST,\n",
    "        catalog_limit=catalog_limit,\n",
    "        schema_limit_per_catalog=schema_limit_per_catalog,\n",
    "        max_workers=max_workers,\n",
    "        spark=spark,\n",
    "        write_callback=write_callback\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "unity_catalog",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
