{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58c9ed36-a3a6-46b7-8bea-e4626061aeac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Unity Catalog Enumeration Logic\n",
    "\n",
    "This notebook provides utilities for enumerating Unity Catalog objects (catalogs, schemas, tables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "026c25fe-4cb6-4cb5-b213-1e80949d8878",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Library Imports"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import threading\n",
    "from typing import Dict, List, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83a2dc85-74ac-47a2-96f0-b9c1055abea2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Verbose Logging Functions"
    }
   },
   "outputs": [],
   "source": [
    "def log(msg: str):\n",
    "    if VERBOSE_LOG:\n",
    "        print(msg)\n",
    "\n",
    "def banner(txt: str):\n",
    "    print(\"\\n\" + \"=\"*22 + f\" {txt} \" + \"=\"*22 + \"\\n\")\n",
    "\n",
    "def keep_spark_alive(spark, stop_event, interval=60):\n",
    "    \"\"\"\n",
    "    Keep Spark session alive by running simple queries periodically.\n",
    "    Run this in a background thread during long operations to prevent session timeout.\n",
    "    \n",
    "    Args:\n",
    "        spark: SparkSession instance\n",
    "        stop_event: threading.Event to signal when to stop\n",
    "        interval: Seconds between keep-alive pings (default: 60)\n",
    "    \"\"\"\n",
    "    while not stop_event.is_set():\n",
    "        try:\n",
    "            # Simple operation to keep session alive\n",
    "            spark.sql(\"SELECT 1\").collect()\n",
    "            time.sleep(interval)\n",
    "        except Exception as e:\n",
    "            if VERBOSE_LOG:\n",
    "                print(f\"[KEEP-ALIVE] Warning: {e}\")\n",
    "            time.sleep(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2472967e-9773-4b45-89a5-4b07aaf17d1f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Unity Catalog Enumerator Class"
    }
   },
   "outputs": [],
   "source": [
    "class UnityCatalogEnumerator:\n",
    "    \"\"\"Unity Catalog enumeration with threading and limits.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, headers: Dict[str, str]):\n",
    "        self.base_url = base_url\n",
    "        self.headers = headers\n",
    "    \n",
    "    def list_tables_for_schema(self, catalog: str, schema: str) -> Tuple[int, int, int]:\n",
    "        \"\"\"Return (tables, managed, external) for catalog.schema\"\"\"\n",
    "        try:\n",
    "            r = requests.get(\n",
    "                f\"{self.base_url}/api/2.1/unity-catalog/tables\",\n",
    "                headers=self.headers,\n",
    "                params={\"catalog_name\": catalog, \"schema_name\": schema},\n",
    "                timeout=HTTP_TIMEOUT_SEC\n",
    "            )\n",
    "            if r.status_code == 200:\n",
    "                tables = r.json().get(\"tables\", [])\n",
    "                managed = sum(1 for t in tables if t.get(\"table_type\") == \"MANAGED\")\n",
    "                external = sum(1 for t in tables if t.get(\"table_type\") == \"EXTERNAL\")\n",
    "                return len(tables), managed, external\n",
    "        except Exception as e:\n",
    "            log(f\"[UC-ERROR] {catalog}.{schema}: {e}\")\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    def list_tables_for_schema_with_data(self, catalog: str, schema: str) -> List[Dict[str, any]]:\n",
    "        \"\"\"Return list of table metadata for catalog.schema\"\"\"\n",
    "        try:\n",
    "            r = requests.get(\n",
    "                f\"{self.base_url}/api/2.1/unity-catalog/tables\",\n",
    "                headers=self.headers,\n",
    "                params={\"catalog_name\": catalog, \"schema_name\": schema},\n",
    "                timeout=HTTP_TIMEOUT_SEC\n",
    "            )\n",
    "            if r.status_code == 200:\n",
    "                tables = r.json().get(\"tables\", [])\n",
    "                # Add catalog and schema context to each table\n",
    "                for tbl in tables:\n",
    "                    tbl[\"_catalog\"] = catalog\n",
    "                    tbl[\"_schema\"] = schema\n",
    "                return tables\n",
    "        except Exception as e:\n",
    "            log(f\"[UC-ERROR] {catalog}.{schema}: {e}\")\n",
    "        return []\n",
    "    \n",
    "    def get_catalogs(self, allowlist: List[str] = None, catalog_limit: int = 0) -> List[str]:\n",
    "        \"\"\"Get list of catalogs to scan.\"\"\"\n",
    "        try:\n",
    "            rr = requests.get(f\"{self.base_url}/api/2.1/unity-catalog/catalogs\", \n",
    "                            headers=self.headers, timeout=HTTP_TIMEOUT_SEC)\n",
    "            rr.raise_for_status()\n",
    "            catalogs = [c[\"name\"] for c in rr.json().get(\"catalogs\", [])]\n",
    "            \n",
    "            if allowlist:\n",
    "                # Special case: if hive_metastore is in allowlist, always include it\n",
    "                # (it's not returned by catalogs API but exists in legacy metastore)\n",
    "                if \"hive_metastore\" in allowlist and \"hive_metastore\" not in catalogs:\n",
    "                    catalogs.append(\"hive_metastore\")\n",
    "                    log(\"[UC] Added legacy hive_metastore to catalog list\")\n",
    "                \n",
    "                catset = set(catalogs)\n",
    "                catalogs = [c for c in allowlist if c in catset]\n",
    "            \n",
    "            catalogs = sorted(catalogs)\n",
    "            \n",
    "            if catalog_limit and catalog_limit > 0:\n",
    "                catalogs = catalogs[:catalog_limit]\n",
    "                \n",
    "            return catalogs\n",
    "        except Exception as e:\n",
    "            log(f\"[UC-ERROR] Failed to get catalogs: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_schemas_for_catalog(self, catalog: str, schema_limit: int = 0) -> List[str]:\n",
    "        \"\"\"Get list of schemas for a catalog.\"\"\"\n",
    "        try:\n",
    "            rs = requests.get(\n",
    "                f\"{self.base_url}/api/2.1/unity-catalog/schemas\",\n",
    "                headers=self.headers,\n",
    "                params={\"catalog_name\": catalog},\n",
    "                timeout=HTTP_TIMEOUT_SEC\n",
    "            )\n",
    "            if rs.status_code != 200:\n",
    "                log(f\"[UC] Skip catalog {catalog} (status={rs.status_code})\")\n",
    "                return []\n",
    "            \n",
    "            schemas = [s[\"name\"] for s in rs.json().get(\"schemas\", [])]\n",
    "            schemas = sorted(schemas)\n",
    "            \n",
    "            if schema_limit and schema_limit > 0:\n",
    "                schemas = schemas[:schema_limit]\n",
    "                \n",
    "            return schemas\n",
    "        except Exception as e:\n",
    "            log(f\"[UC-ERROR] Failed to get schemas for {catalog}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def enumerate_unity_catalog(\n",
    "        self,\n",
    "        enable: bool = True,\n",
    "        allowlist: List[str] = None,\n",
    "        catalog_limit: int = 0,\n",
    "        schema_limit_per_catalog: int = 0,\n",
    "        max_workers: int = 20,\n",
    "        write_callback = None,\n",
    "        batch_size: int = 5000,\n",
    "        spark = None\n",
    "    ) -> Tuple[Dict[str, int], Dict[str, List[Dict[str, any]]]]:\n",
    "        \"\"\"\n",
    "        Enumerate Unity Catalog with configurable limits and threading.\n",
    "        \n",
    "        Args:\n",
    "            write_callback: Optional function(table_type, records) to write batches periodically\n",
    "            batch_size: Write every N tables (only used if write_callback is provided)\n",
    "        \n",
    "        Returns tuple of (counts, raw_data):\n",
    "        - counts: dict with uc_catalogs, uc_schemas, uc_tables, managed_tables, external_tables\n",
    "        - raw_data: dict with \"schemas\" and \"tables\" lists containing full metadata\n",
    "                    (empty if write_callback is used for tables)\n",
    "        \"\"\"\n",
    "        banner(\"3/4 Unity Catalog Enumeration\")\n",
    "        out = {\n",
    "            \"uc_catalogs\": 0,\n",
    "            \"uc_schemas\": 0, \n",
    "            \"uc_tables\": 0,\n",
    "            \"managed_tables\": 0,\n",
    "            \"external_tables\": 0\n",
    "        }\n",
    "        \n",
    "        # Collect actual data\n",
    "        raw_data = {\n",
    "            \"schemas\": [],\n",
    "            \"tables\": []\n",
    "        }\n",
    "        \n",
    "        if not enable:\n",
    "            print(\"[UC] Skipped (UC_ENABLE=False).\")\n",
    "            return out, raw_data\n",
    "\n",
    "        # Start keep-alive thread to prevent Spark session timeout\n",
    "        stop_keep_alive = threading.Event()\n",
    "        keep_alive_thread = None\n",
    "        \n",
    "        if spark:\n",
    "            keep_alive_thread = threading.Thread(\n",
    "                target=keep_spark_alive,\n",
    "                args=(spark, stop_keep_alive, 60),\n",
    "                daemon=True\n",
    "            )\n",
    "            keep_alive_thread.start()\n",
    "            print(\"[UC] Spark keep-alive thread started (60s interval)\")\n",
    "\n",
    "        try:\n",
    "            allowlist = allowlist or []\n",
    "            \n",
    "            # Get catalogs to scan\n",
    "            catalogs = self.get_catalogs(allowlist, catalog_limit)\n",
    "            if not catalogs:\n",
    "                print(\"[UC] No catalogs found or accessible.\")\n",
    "                return out, raw_data\n",
    "                \n",
    "            out[\"uc_catalogs\"] = len(catalogs)\n",
    "\n",
    "            print(f\"[UC] Scope: allowlist={allowlist or 'ALL'}, cat_limit={catalog_limit or 'ALL'}, schema_limit={schema_limit_per_catalog or 'ALL'}\")\n",
    "            print(f\"[UC] Catalogs to scan: {len(catalogs)} → {catalogs[:5]}{' …' if len(catalogs)>5 else ''}\")\n",
    "\n",
    "            total_schemas = total_tables = managed = external = 0\n",
    "            futures = []\n",
    "            queued_catalogs = 0\n",
    "            table_buffer = []  # Buffer for batch writing\n",
    "\n",
    "            with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "                # Queue up all schema-table enumeration tasks\n",
    "                for cat in catalogs:\n",
    "                    schemas = self.get_schemas_for_catalog(cat, schema_limit_per_catalog)\n",
    "                    total_schemas += len(schemas)\n",
    "                    print(f\"[UC] {cat}: {len(schemas)} schemas (limit={schema_limit_per_catalog or 'ALL'})\")\n",
    "                    \n",
    "                    # Capture schema data\n",
    "                    for s in schemas:\n",
    "                        raw_data[\"schemas\"].append({\n",
    "                            \"catalog_name\": cat,\n",
    "                            \"schema_name\": s,\n",
    "                            \"full_name\": f\"{cat}.{s}\"\n",
    "                        })\n",
    "                        # Queue table enumeration with full data\n",
    "                        futures.append(ex.submit(self.list_tables_for_schema_with_data, cat, s))\n",
    "                        time.sleep(0.002)  # Small delay to avoid overwhelming the API\n",
    "                    \n",
    "                    queued_catalogs += 1\n",
    "                    if queued_catalogs % 5 == 0:\n",
    "                        print(f\"[UC] Heartbeat: queued {queued_catalogs}/{len(catalogs)} catalogs\")\n",
    "\n",
    "                # Process results as they complete\n",
    "                for i, f in enumerate(as_completed(futures), 1):\n",
    "                    table_data = f.result()\n",
    "                    \n",
    "                    if table_data:\n",
    "                        # Add to buffer instead of directly to raw_data\n",
    "                        table_buffer.extend(table_data)\n",
    "                        \n",
    "                        # Count stats\n",
    "                        for tbl in table_data:\n",
    "                            total_tables += 1\n",
    "                            if tbl.get(\"table_type\") == \"MANAGED\":\n",
    "                                managed += 1\n",
    "                            elif tbl.get(\"table_type\") == \"EXTERNAL\":\n",
    "                                external += 1\n",
    "                        \n",
    "                        # Write batch when buffer reaches threshold\n",
    "                        if len(table_buffer) >= batch_size and write_callback:\n",
    "                            print(f\"[UC] Writing batch of {len(table_buffer)} tables to Delta (total so far: {total_tables})...\")\n",
    "                            write_callback(\"databricks_table\", table_buffer)\n",
    "                            table_buffer = []  # Clear buffer after write\n",
    "                    \n",
    "                    if i % 200 == 0:\n",
    "                        print(f\"[UC] Heartbeat: processed {i} schema-table batches (tables so far={total_tables})\")\n",
    "                \n",
    "                # Write remaining tables in buffer\n",
    "                if table_buffer and write_callback:\n",
    "                    print(f\"[UC] Writing final batch of {len(table_buffer)} tables to Delta...\")\n",
    "                    write_callback(\"databricks_table\", table_buffer)\n",
    "                    table_buffer = []\n",
    "\n",
    "            out[\"uc_schemas\"] = total_schemas\n",
    "            out[\"uc_tables\"] = total_tables\n",
    "            out[\"managed_tables\"] = managed\n",
    "            out[\"external_tables\"] = external\n",
    "\n",
    "            # Build scope description\n",
    "            scope = []\n",
    "            if allowlist: \n",
    "                scope.append(f\"allow={allowlist}\")\n",
    "            if catalog_limit: \n",
    "                scope.append(f\"cat_limit={catalog_limit}\")\n",
    "            if schema_limit_per_catalog: \n",
    "                scope.append(f\"schema_limit={schema_limit_per_catalog}\")\n",
    "            desc = \", \".join(scope) if scope else \"full-scan\"\n",
    "            \n",
    "            # For backward compatibility, only return table data if no callback was used\n",
    "            if not write_callback and table_buffer:\n",
    "                raw_data[\"tables\"] = table_buffer\n",
    "            \n",
    "            print(f\"[UC] Done ({desc}) → {out['uc_catalogs']} catalogs, {out['uc_schemas']} schemas, {out['uc_tables']} tables\")\n",
    "            if write_callback:\n",
    "                print(f\"[UC] Collected {len(raw_data['schemas'])} schema records, tables written in batches\")\n",
    "            else:\n",
    "                print(f\"[UC] Collected {len(raw_data['schemas'])} schema records, {len(raw_data['tables'])} table records\")\n",
    "            return out, raw_data\n",
    "        \n",
    "        finally:\n",
    "            # Stop keep-alive thread\n",
    "            if spark and keep_alive_thread:\n",
    "                stop_keep_alive.set()\n",
    "                keep_alive_thread.join(timeout=5)\n",
    "                print(\"[UC] Spark keep-alive thread stopped\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1ac6018-416a-48ce-a48d-5aa47cc878e5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Enumerate UC Function"
    }
   },
   "outputs": [],
   "source": [
    "# Convenience function for backward compatibility\n",
    "def enumerate_uc(\n",
    "    base_url: str,\n",
    "    headers: Dict[str, str],\n",
    "    enable: bool = UC_ENABLE,\n",
    "    allowlist: List[str] = None,\n",
    "    catalog_limit: int = UC_CATALOG_LIMIT,\n",
    "    schema_limit_per_catalog: int = UC_SCHEMA_LIMIT_PER_CATALOG,\n",
    "    max_workers: int = UC_MAX_WORKERS,\n",
    "    write_callback = None,\n",
    "    batch_size: int = 5000,\n",
    "    spark = None\n",
    ") -> Tuple[Dict[str, int], Dict[str, List[Dict[str, any]]]]:\n",
    "    \"\"\"\n",
    "    Wrapper for Unity Catalog enumeration.\n",
    "    \n",
    "    Args:\n",
    "        write_callback: Optional function(table_type, records) to write batches periodically\n",
    "        batch_size: Write every N tables (only used if write_callback is provided)\n",
    "        spark: SparkSession to keep alive during long enumeration\n",
    "    \n",
    "    Returns tuple of (counts, raw_data):\n",
    "    - counts: dict with catalog/schema/table counts\n",
    "    - raw_data: dict with \"schemas\" and \"tables\" lists containing full metadata\n",
    "    \"\"\"\n",
    "    enumerator = UnityCatalogEnumerator(base_url, headers)\n",
    "    return enumerator.enumerate_unity_catalog(\n",
    "        enable=enable,\n",
    "        allowlist=allowlist or UC_CATALOG_ALLOWLIST,\n",
    "        catalog_limit=catalog_limit,\n",
    "        schema_limit_per_catalog=schema_limit_per_catalog,\n",
    "        max_workers=max_workers,\n",
    "        write_callback=write_callback,\n",
    "        batch_size=batch_size,\n",
    "        spark=spark\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "unity_catalog",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
