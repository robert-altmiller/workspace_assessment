{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ad54770-e675-4194-bb9e-3640aabe1081",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks API Client with Async HTTP and Pagination\n",
    "\n",
    "This notebook provides an async HTTP client for Databricks REST APIs with pagination support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "640fb9e7-82d1-4ec3-aad3-c02b068eab88",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Library Imports"
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "from typing import Dict, Any, List, Tuple\n",
    "import aiohttp\n",
    "from aiohttp import TCPConnector, ClientTimeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dd0b95f-70e8-4891-a658-79d6b49a4794",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Databricks API Client Class"
    }
   },
   "outputs": [],
   "source": [
    "class DatabricksAPIClient:\n",
    "\n",
    "    def __init__(self, workspace_url: str, token: str, data_processor=None, enable_streaming_writes=False):\n",
    "        self.base_url = f\"https://{workspace_url}\"\n",
    "        self.headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "        self.workspace_url = workspace_url\n",
    "        self.semaphore = asyncio.Semaphore(MAX_CONCURRENCY)\n",
    "        self.data_processor = data_processor\n",
    "        self.enable_streaming_writes = enable_streaming_writes\n",
    "        \n",
    "\n",
    "    def log(self, msg: str):\n",
    "        if VERBOSE_LOG:\n",
    "            print(msg)\n",
    "    \n",
    "\n",
    "    def _print_http(self, url, status):\n",
    "        if DEBUG_HTTP:\n",
    "            print(f\"[HTTP] GET {url} â†’ {status}\")\n",
    "\n",
    "\n",
    "    async def _safe_get(self, session: aiohttp.ClientSession, url: str, params: Dict[str, Any] = None) -> Tuple[int, Any]:\n",
    "        \"\"\"Make a safe HTTP GET request with retries and rate limiting.\"\"\"\n",
    "        params = params or {}\n",
    "        for attempt in range(RETRY_ATTEMPTS):\n",
    "            try:\n",
    "                async with session.get(url, headers=self.headers, timeout=HTTP_TIMEOUT_SEC, params=params) as r:\n",
    "                    self._print_http(r.url, r.status)\n",
    "                    if r.status == 429:\n",
    "                        wait = RETRY_DELAY_BASE * (2 ** attempt)\n",
    "                        self.log(f\"[WARN] Rate limited: {r.url} (sleep {wait}s)\")\n",
    "                        await asyncio.sleep(wait)\n",
    "                        continue\n",
    "                    if r.status in (200, 201):\n",
    "                        return r.status, await r.json()\n",
    "                    # common non-retryables\n",
    "                    if r.status in (400, 403, 404):\n",
    "                        text = await r.text()\n",
    "                        self.log(f\"[ERROR] {r.status}: {r.url} â†’ {text[:140]} ...\")\n",
    "                        return r.status, {}\n",
    "                    text = await r.text()\n",
    "                    self.log(f\"[ERROR] {r.status}: {r.url} â†’ {text[:140]} ...\")\n",
    "                    return r.status, {}\n",
    "            except Exception as e:\n",
    "                self.log(f\"[EXC] {url} params={params}: {e}\")\n",
    "                await asyncio.sleep(RETRY_DELAY_BASE)\n",
    "        return 429, {}\n",
    "\n",
    "\n",
    "    async def _paginate(self, session, base_url, cfg, endpoint_key: str = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Generic paginator supporting next_page_token/page_token + has_more.\"\"\"\n",
    "        items: List[Dict[str, Any]] = []\n",
    "        params = dict(cfg.get(\"fixed_params\") or {})\n",
    "        limit_param = cfg.get(\"limit_param\")\n",
    "        token_key   = cfg.get(\"token_key\")\n",
    "        list_key    = cfg.get(\"list_key\")\n",
    "        limit_val   = cfg.get(\"limit\", PAGE_SIZE_DEFAULT)\n",
    "\n",
    "        if limit_param and limit_val:\n",
    "            params[limit_param] = limit_val\n",
    "\n",
    "        page_num = 0\n",
    "        \n",
    "        while True:\n",
    "            page_num += 1\n",
    "            status, data = await self._safe_get(session, base_url, params=params)\n",
    "            if status != 200 or not isinstance(data, dict):\n",
    "                break\n",
    "\n",
    "            if list_key and isinstance(data.get(list_key), list):\n",
    "                page_items = data[list_key]\n",
    "                items.extend(page_items)\n",
    "                \n",
    "                # Log pagination progress for each page (skip page 1 to reduce noise)\n",
    "                if VERBOSE_LOG and page_num > 1:\n",
    "                    nxt = data.get(token_key) if token_key else None\n",
    "                    if nxt:\n",
    "                        print(f\"      ðŸ“„ Page {page_num}: {len(page_items)} items | Next token: {nxt[:30]}...\")\n",
    "                    else:\n",
    "                        print(f\"      ðŸ“„ Page {page_num}: {len(page_items)} items | Last page\")\n",
    "                        \n",
    "            elif list_key is None:\n",
    "                break  # object/unsupported\n",
    "\n",
    "            # one-page throttle for dashboards when pagination is disabled\n",
    "            if not cfg.get(\"paginate\", False) and \"/dashboards\" in base_url:\n",
    "                break\n",
    "\n",
    "            nxt = data.get(token_key) if token_key else None\n",
    "            if nxt:\n",
    "                # Use page_param for request if defined, otherwise use token_key\n",
    "                page_param = cfg.get(\"page_param\", token_key)\n",
    "                params[page_param] = nxt\n",
    "            else:\n",
    "                has_more = data.get(\"has_more\") or data.get(\"has_next_page\") or False\n",
    "                if not has_more:\n",
    "                    break\n",
    "\n",
    "        return items\n",
    "\n",
    "    def _write_raw_data_immediately(self, key: str, rows: List[Dict[str, Any]]) -> int:\n",
    "        \"\"\"Write raw data immediately to Unity Catalog if streaming writes are enabled.\"\"\"\n",
    "        if not self.enable_streaming_writes or not self.data_processor:\n",
    "            return len(rows)\n",
    "        \n",
    "        try:\n",
    "            self.data_processor.write_single_raw_table(key, rows)\n",
    "            self.log(f\"[STREAM] {key:35} â†’ UC written immediately ({len(rows)} rows)\")\n",
    "            return len(rows)\n",
    "        except Exception as e:\n",
    "            self.log(f\"[STREAM-FAIL] {key:35} â†’ {e}\")\n",
    "            return len(rows)\n",
    "\n",
    "    async def fetch_endpoint(self, session, key, cfg) -> Tuple[str, List[Dict[str, Any]]]:\n",
    "        \"\"\"Fetch data from a single endpoint.\"\"\"\n",
    "        t0 = time.time()\n",
    "        url = f\"{self.base_url}{cfg['url']}\"\n",
    "\n",
    "        # Skip vector search indexes here - they're collected separately after endpoints\n",
    "        if key == \"databricks_vector_search_index\":\n",
    "            return key, []\n",
    "\n",
    "        # Paginated path\n",
    "        if cfg.get(\"paginate\", False):\n",
    "            rows = await self._paginate(session, url, cfg, endpoint_key=key)\n",
    "            dt = time.time() - t0\n",
    "            self.log(f\"[Fetch] {key:35} âœ… ({len(rows)})  â† paginated:{cfg.get('list_key')}  [{dt:.1f}s]\")\n",
    "            self._write_raw_data_immediately(key, rows)\n",
    "            return key, rows\n",
    "\n",
    "        # One-shot\n",
    "        status, data = await self._safe_get(session, url, cfg.get(\"fixed_params\"))\n",
    "        dt = time.time() - t0\n",
    "        if status != 200:\n",
    "            self.log(f\"[Fetch] {key:35} âŒ (status={status})  [{dt:.1f}s]\")\n",
    "            return key, []\n",
    "\n",
    "        list_key = cfg.get(\"list_key\")\n",
    "        if list_key and isinstance(data, dict) and isinstance(data.get(list_key), list):\n",
    "            rows = data[list_key]\n",
    "            self.log(f\"[Fetch] {key:35} âœ… ({len(rows)})  â† {list_key}  [{dt:.1f}s]\")\n",
    "            self._write_raw_data_immediately(key, rows)\n",
    "            return key, rows\n",
    "\n",
    "        # workspace-conf returns an object\n",
    "        if key == \"databricks_workspace_conf\" and isinstance(data, dict):\n",
    "            rows = [data]\n",
    "            self.log(f\"[Fetch] {key:35} âœ… ({len(rows)})  â† object  [{dt:.1f}s]\")\n",
    "            self._write_raw_data_immediately(key, rows)\n",
    "            return key, rows\n",
    "\n",
    "        # groups endpoint may return group_names: [str]\n",
    "        if key == \"databricks_group\" and isinstance(data, dict) and isinstance(data.get(\"group_names\"), list):\n",
    "            rows = [{\"group_name\": g} for g in data[\"group_names\"]]\n",
    "            self.log(f\"[Fetch] {key:35} âœ… ({len(rows)})  â† group_names  [{dt:.1f}s]\")\n",
    "            self._write_raw_data_immediately(key, rows)\n",
    "            return key, rows\n",
    "\n",
    "        # default: first list found\n",
    "        if isinstance(data, dict):\n",
    "            for k, v in data.items():\n",
    "                if isinstance(v, list):\n",
    "                    rows = v\n",
    "                    self.log(f\"[Fetch] {key:35} âœ… ({len(rows)})  â† {k}  [{dt:.1f}s]\")\n",
    "                    self._write_raw_data_immediately(key, rows)\n",
    "                    return key, rows\n",
    "        if isinstance(data, list):\n",
    "            rows = data\n",
    "            self.log(f\"[Fetch] {key:35} âœ… ({len(rows)})  â† list  [{dt:.1f}s]\")\n",
    "            self._write_raw_data_immediately(key, rows)\n",
    "            return key, rows\n",
    "\n",
    "        self.log(f\"[Fetch] {key:35} âœ… (0)  [{dt:.1f}s]\")\n",
    "        return key, []\n",
    "\n",
    "\n",
    "    async def fetch_one(self, session, key, cfg):\n",
    "        \"\"\"Fetch one endpoint with semaphore control.\"\"\"\n",
    "        async with self.semaphore:\n",
    "            return await self.fetch_endpoint(session, key, cfg)\n",
    "\n",
    "\n",
    "    async def fetch_vector_search_indexes(self, session, endpoints_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Fetch all vector search indexes for all endpoints.\"\"\"\n",
    "        all_indexes = []\n",
    "        \n",
    "        if not endpoints_data:\n",
    "            return all_indexes\n",
    "        \n",
    "        print(f\"\\n[VS] Fetching indexes for {len(endpoints_data)} vector search endpoints...\")\n",
    "        \n",
    "        for idx, endpoint in enumerate(endpoints_data, 1):\n",
    "            endpoint_name = endpoint.get(\"name\")\n",
    "            if not endpoint_name:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                url = f\"{self.base_url}/api/2.0/vector-search/indexes\"\n",
    "                params = {\"endpoint_name\": endpoint_name, \"page_size\": PAGE_SIZE_DEFAULT}\n",
    "                \n",
    "                # Paginate through indexes for this endpoint\n",
    "                page_num = 0\n",
    "                while True:\n",
    "                    page_num += 1\n",
    "                    status, data = await self._safe_get(session, url, params=params)\n",
    "                    \n",
    "                    if status != 200 or not isinstance(data, dict):\n",
    "                        break\n",
    "                    \n",
    "                    indexes = data.get(\"vector_indexes\", [])\n",
    "                    if indexes:\n",
    "                        # Add parent endpoint reference\n",
    "                        for idx_item in indexes:\n",
    "                            idx_item[\"_endpoint_name\"] = endpoint_name\n",
    "                        all_indexes.extend(indexes)\n",
    "                    \n",
    "                    next_token = data.get(\"next_page_token\")\n",
    "                    if next_token:\n",
    "                        params[\"page_token\"] = next_token\n",
    "                    else:\n",
    "                        break\n",
    "                \n",
    "                # Progress update every 20 endpoints\n",
    "                if idx % 20 == 0 or idx == len(endpoints_data):\n",
    "                    print(f\"   [VS] Progress: {idx}/{len(endpoints_data)} endpoints processed, {len(all_indexes)} indexes found\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                if VERBOSE_LOG:\n",
    "                    self.log(f\"[VS-ERROR] {endpoint_name}: {e}\")\n",
    "        \n",
    "        print(f\"[VS] âœ… Found {len(all_indexes)} total indexes across {len(endpoints_data)} endpoints\\n\")\n",
    "        return all_indexes\n",
    "\n",
    "\n",
    "    async def collect_all_endpoints(self) -> Tuple[Dict[str, List[Dict[str, Any]]], Dict[str, int]]:\n",
    "        \"\"\"Collect data from all configured endpoints asynchronously.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*22 + \" 1/4 Async Collect (REST) \" + \"=\"*22 + \"\\n\")\n",
    "        raw: Dict[str, List[Dict[str, Any]]] = {}\n",
    "        counts: Dict[str, int] = {}\n",
    "\n",
    "        timeout = ClientTimeout(total=None, sock_connect=HTTP_TIMEOUT_SEC, sock_read=HTTP_TIMEOUT_SEC)\n",
    "        connector = TCPConnector(limit=MAX_CONCURRENCY, ttl_dns_cache=300)\n",
    "\n",
    "        all_endpoints = get_all_endpoints()\n",
    "        \n",
    "        # Split into parallel and sequential\n",
    "        parallel = {k: v for k, v in all_endpoints.items() if k not in SEQUENTIAL_ENDPOINTS}\n",
    "        sequential = {k: v for k, v in all_endpoints.items() if k in SEQUENTIAL_ENDPOINTS}\n",
    "\n",
    "        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:\n",
    "            \n",
    "            # Phase 1: Process parallel endpoints first (fast completion)\n",
    "            if parallel:\n",
    "                print(f\"[PARALLEL] Processing {len(parallel)} fast endpoints with concurrency={MAX_CONCURRENCY}...\\n\")\n",
    "                task_by_key = {k: asyncio.create_task(self.fetch_one(session, k, v)) for k, v in parallel.items()}\n",
    "\n",
    "                async def heartbeat():\n",
    "                    start_time = time.time()\n",
    "                    last_done = 0\n",
    "                    repeat_count = 0\n",
    "                    \n",
    "                    while True:\n",
    "                        done = sum(t.done() for t in task_by_key.values())\n",
    "                        total = len(task_by_key)\n",
    "                        inflight = [k for k, t in task_by_key.items() if not t.done()]\n",
    "                        elapsed = int(time.time() - start_time)\n",
    "                        \n",
    "                        # Show progress when something changes\n",
    "                        if done != last_done:\n",
    "                            print(f\"[HB] {elapsed}s | {done}/{total} completed | Now fetching: {inflight[:3]}\")\n",
    "                            repeat_count = 0\n",
    "                            last_done = done\n",
    "                        else:\n",
    "                            # Still working on same endpoints - only print periodically\n",
    "                            repeat_count += 1\n",
    "                            if repeat_count % 3 == 0:  # Every 15 seconds (if HEARTBEAT_SEC=5)\n",
    "                                print(f\"[HB] {elapsed}s | Still working: {inflight[:3]} ...\")\n",
    "                        \n",
    "                        await asyncio.sleep(HEARTBEAT_SEC)\n",
    "\n",
    "                hb = asyncio.create_task(heartbeat())\n",
    "                results = await asyncio.gather(*task_by_key.values())\n",
    "                hb.cancel()\n",
    "\n",
    "                for key, rows in results:\n",
    "                    raw[key] = [] if self.enable_streaming_writes else rows\n",
    "                    counts[key] = len(rows)\n",
    "                \n",
    "                print(f\"\\n[PARALLEL] âœ… Completed {len(parallel)} endpoints\\n\")\n",
    "            \n",
    "            # Phase 2: Process sequential endpoints one at a time (slow, large datasets)\n",
    "            if sequential:\n",
    "                print(f\"[SEQUENTIAL] Now processing {len(sequential)} large endpoints individually...\\n\")\n",
    "                for idx, (key, cfg) in enumerate(sequential.items(), 1):\n",
    "                    print(f\"[SEQ {idx}/{len(sequential)}] Fetching {key}...\")\n",
    "                    result_key, rows = await self.fetch_endpoint(session, key, cfg)\n",
    "                    raw[result_key] = [] if self.enable_streaming_writes else rows\n",
    "                    counts[result_key] = len(rows)\n",
    "                    print(f\"[SEQ {idx}/{len(sequential)}] âœ… {key} complete ({len(rows)} rows)\\n\")\n",
    "            \n",
    "            # Phase 3: Fetch vector search indexes (requires endpoints to be collected first)\n",
    "            if \"databricks_vector_search_endpoint\" in counts and counts.get(\"databricks_vector_search_endpoint\", 0) > 0:\n",
    "                # Always re-fetch endpoints data since we need the endpoint names\n",
    "                vs_cfg = all_endpoints.get(\"databricks_vector_search_endpoint\", {})\n",
    "                _, vs_endpoints = await self.fetch_endpoint(session, \"databricks_vector_search_endpoint\", vs_cfg)\n",
    "                \n",
    "                indexes = await self.fetch_vector_search_indexes(session, vs_endpoints)\n",
    "                raw[\"databricks_vector_search_index\"] = [] if self.enable_streaming_writes else indexes\n",
    "                counts[\"databricks_vector_search_index\"] = len(indexes)\n",
    "                \n",
    "                if self.enable_streaming_writes and indexes:\n",
    "                    self._write_raw_data_immediately(\"databricks_vector_search_index\", indexes)\n",
    "\n",
    "        print(\"[Async] Collection complete.\\n\")\n",
    "        return raw, counts\n",
    "\n",
    "\n",
    "    def collect_dbfs_mounts(self, dbutils=None) -> Tuple[List[Dict[str, Any]], int]:\n",
    "        \"\"\"Collect DBFS mount points (requires dbutils, not REST API).\"\"\"\n",
    "        try:\n",
    "            if dbutils is None:\n",
    "                # dbutils should be passed from Databricks notebook environment\n",
    "                raise ImportError(\"dbutils not available - must be run in Databricks environment\")\n",
    "            \n",
    "            mounts = [m.path for m in dbutils.fs.mounts()]\n",
    "            mount_data = [{\"mount_point\": p} for p in mounts]\n",
    "            self.log(f\"[DBFS] Mounts: {len(mounts)}\")\n",
    "            return mount_data, len(mounts)\n",
    "        except Exception as e:\n",
    "            self.log(f\"[DBFS-FAIL] {e}\")\n",
    "            return [], 0"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "api_client",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
